{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12525e88",
   "metadata": {},
   "source": [
    "# FIle Structure\n",
    "- `origin_data`: From DB 的原始資料\n",
    "- `origin_data_csv`: 原始資料轉檔為 `.csv`\n",
    "- `logs`: 實驗輸出檔案資料夾\n",
    "- `Z:` : 網路磁碟機(WFDB NAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8487073",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN_DATA = \"origin_data\"\n",
    "DATA_CSV = \"origin_data_csv\"\n",
    "LOGS = \"logs\"\n",
    "MATCH = \"Z:\"\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import wfdb                                      # 讀取 WFDB header / record :contentReference[oaicite:4]{index=4}\n",
    "from pathlib import Path                         # 物件導向檔案操作 :contentReference[oaicite:5]{index=5}\n",
    "from datetime import datetime, timedelta, date, time\n",
    "from tqdm import tqdm                            # 進度列（可省略）\n",
    "import logging, os                               # 紀錄檔與系統路徑\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Optional, Tuple,Set\n",
    "from  tqdm import tqdm\n",
    "\n",
    "def cross_validation_missing_subject(fileA: str | pd.DataFrame, fileB: str | pd.DataFrame, fileAname:str, fileBname:str):\n",
    "    \"\"\"\n",
    "    列出\n",
    "    - 哪些 subject id 存在於 fileA 但不存在於 fileB\n",
    "    - 哪些 subject id 存在於 fileB 但不存在於 fileA\n",
    "    並將結果列出\n",
    "\n",
    "    Arg:\n",
    "    - fileA: file path of fileA\n",
    "    - fileB: file path of fileB\n",
    "    \"\"\"\n",
    "    if not isinstance(fileA,pd.DataFrame):\n",
    "        fileA_df = pd.read_csv(fileA)\n",
    "    else:\n",
    "        fileA_df = fileA\n",
    "\n",
    "    if not isinstance(fileB,pd.DataFrame):\n",
    "        fileB_df = pd.read_csv(fileB)\n",
    "    else:\n",
    "        fileB_df = fileB\n",
    "        \n",
    "\n",
    "    # 確認欄位名稱（假設欄位叫 SUBJECT_ID）\n",
    "    if 'SUBJECT_ID' not in fileA_df.columns or 'SUBJECT_ID' not in fileB_df.columns:\n",
    "        raise ValueError(\"Both files must contain 'SUBJECT_ID' column\")\n",
    "\n",
    "    # 轉換為集合\n",
    "    setA = set(fileA_df['SUBJECT_ID'].dropna().astype(str))\n",
    "    setB = set(fileB_df['SUBJECT_ID'].dropna().astype(str))\n",
    "\n",
    "    # 找差集\n",
    "    only_in_A = setA - setB\n",
    "    only_in_B = setB - setA\n",
    "\n",
    "    print(f\"✅ SUBJECT_ID 存在於 {fileAname} 但不存在於 {fileBname}, 共 {len(only_in_A)}:\")\n",
    "    print(only_in_A if only_in_A else \"無\")\n",
    "    \n",
    "    print(f\"\\n✅ SUBJECT_ID 存在於 {fileBname} 但不存在於 {fileAname}共 {len(only_in_B)}:\")\n",
    "    print(only_in_B if only_in_B else \"無\")\n",
    "\n",
    "    # 回傳結果（以 dict）\n",
    "    return {\n",
    "        \"only_in_A\": only_in_A,\n",
    "        \"only_in_B\": only_in_B\n",
    "    }\n",
    "\n",
    "try:\n",
    "    alive_yuran = pd.read_csv(\"./experiment_data_from_yuran/alive_42731_withHRV.csv\")\n",
    "    dead_yuran = pd.read_csv(\"./experiment_data_from_yuran/dead_42731_withHRV.csv\")\n",
    "\n",
    "    alive_set = set(alive_yuran['SUBJECT_ID'].to_list())\n",
    "    dead_set = set(dead_yuran[\"SUBJECT_ID\"].to_list())\n",
    "\n",
    "    total_set = alive_set | dead_set\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d18988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs folder exist.\n",
      "Logger module create success.\n"
     ]
    }
   ],
   "source": [
    "# ================================= 初始化紀錄 =================================\n",
    "\n",
    "if os.path.isdir(LOGS):\n",
    "    print(f\"{LOGS} folder exist.\")\n",
    "else:\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    print(f\"{LOGS} folder doesn't exist, creating new {LOGS}\")\n",
    "\n",
    "# 建立 Logger\n",
    "try:\n",
    "    logger = logging.getLogger(\"data_clean\")\n",
    "    logger.setLevel(logging.WARNING)  # WARNING 以上都會被記錄\n",
    "\n",
    "    # 建立 FileHandler，寫入 logs/clean.log\n",
    "    fh = logging.FileHandler(f\"{LOGS}/clean.log\", mode=\"w\", encoding=\"utf-8\")\n",
    "    # 只輸出訊息本身：SUBJECT_ID REASON\n",
    "    formatter = logging.Formatter(\"%(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 避免重複加入 handler\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(fh)\n",
    "    print(\"Logger module create success.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(e)\n",
    "\n",
    "def record_log(subject_id: str, reason: str):\n",
    "    \"\"\"\n",
    "    將不合格的資料記錄到 logs/clean.log。\n",
    "    例如： logger.warning(\"12345 invalid_date\")\n",
    "    \"\"\"\n",
    "    logger.warning(f\"{subject_id} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99323088",
   "metadata": {},
   "source": [
    "# Step 1 區分 Survivor Cohart / Deceased Cohart\n",
    "- 具備 ICD Code 42731診斷碼與ICU紀錄的病患\n",
    "    - \"網站說 ECG 一定在 ICU 內\"\n",
    "    - \"有可能沒有 ICU STAY 紀錄\"\n",
    "- 每個subject id 根據 ecg datime只取時間最晚的那一筆\n",
    "    - 定義 `ecg datime只取時間最晚的那一筆`:\n",
    "        - 如果 有多個紀錄的 WFDB header ，只取最後一筆\n",
    "    - 然後ecg datetime一定要在「入出院」時間內，不是icu進出時間\n",
    "        - 入院不管\n",
    "        - 出院或死亡:容忍 30min(ICU OUTTIME)\n",
    "- 根據 `ADMISSION`: hospital_expire_flag 區分死亡與存活\n",
    "    - 死亡時間: Patient.DOD\n",
    "        - 如果 DOD 缺失當作存活(by Yuran), 也有人直接篩掉(by Fish Yang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23c7e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "只取每個病人最後一筆 ICU紀錄\n",
      "總資料:  10252\n",
      "總共多少 subject id 10252\n",
      "Missing ID (total 0): set()\n"
     ]
    }
   ],
   "source": [
    "# ================================= 篩選基本 Clinical Data: base =================================\n",
    "def load_clinical_tables():\n",
    "    \"\"\"\n",
    "    篩選 具備 ICD Code 42731診斷碼與ICU紀錄的病患\n",
    "    Return:\n",
    "    - base : pd.DataFrame, 患者綜合表格，包含以下資訊\n",
    "        來自 patients.csv\n",
    "        - SUBJECT_ID：患者唯一識別碼。\n",
    "\n",
    "        admissions 表格\n",
    "        - HADM_ID：住院請求編號，用於唯一識別一次住院。\n",
    "        - ADMITTIME：病患入院時間（時間戳記）。\n",
    "        - DISCHTIME：病患出院時間（時間戳記）。\n",
    "        - HOSPITAL_EXPIRE_FLAG：出院時是否死亡（1 = 出院時已逝，0 = 否）。\n",
    "        - DEATHTIME: 院內死亡時間\n",
    "\n",
    "        icustays 表格\n",
    "        - ICUSTAY_ID：ICU 住院期間的唯一識別符。\n",
    "        - INTIME：進入 ICU 的時間戳記。\n",
    "        - OUTTIME：離開 ICU 的時間戳記。\n",
    "\n",
    "        diagnoses_icd 表格\n",
    "        - ICD9_CODE：使用 ICD‑9 編碼系統記錄的診斷代碼（最多 6 位字元，包含空格，有些是 V 開頭代碼）\n",
    "    \"\"\"\n",
    "    adm   = pd.read_csv(os.path.join(DATA_CSV,\"ADMISSIONS.csv\"), usecols=['SUBJECT_ID','HADM_ID',\n",
    "                                                        'ADMITTIME','DISCHTIME','DEATHTIME',\n",
    "                                                        'HOSPITAL_EXPIRE_FLAG'])\n",
    "\n",
    "    icu   = pd.read_csv(os.path.join(DATA_CSV,\"ICUSTAYS.csv\"),\n",
    "                        usecols=['SUBJECT_ID','ICUSTAY_ID','HADM_ID','INTIME','OUTTIME'])\n",
    "\n",
    "    diag  = pd.read_csv(os.path.join(DATA_CSV,\"DIAGNOSES_ICD.csv\"),\n",
    "                        usecols=['SUBJECT_ID','HADM_ID','ICD9_CODE'])\n",
    "    # ICD9 42731 = AF\n",
    "    # ── 1. 找出 AF 病人 ───────────────────────\n",
    "    af_subjects = diag.loc[diag['ICD9_CODE']=='42731', 'SUBJECT_ID'].unique()\n",
    "\n",
    "    # ── 2. 保留這些病人的全部 ICU stay ───────\n",
    "    icu_af = icu[icu['SUBJECT_ID'].isin(af_subjects)]\n",
    "\n",
    "    # ── 3. 加入 ADMISSIONS 資料 ───────────────\n",
    "    base = icu_af.merge(adm, on=['SUBJECT_ID', 'HADM_ID'], how='left')\n",
    "\n",
    "    # ── 4. (可選) 標記該次住院是否含 AF ──────\n",
    "    af_flag = (diag[diag['ICD9_CODE']=='42731']\n",
    "            [['SUBJECT_ID','HADM_ID']]\n",
    "            .drop_duplicates()\n",
    "            .assign(HAS_AF=1))\n",
    "    base = base.merge(af_flag,\n",
    "                    on=['SUBJECT_ID','HADM_ID'],\n",
    "                    how='left') \\\n",
    "            .fillna({'HAS_AF': 0})\n",
    "\n",
    "    # ── 5. 去重（若 diag 同 HADM_ID 多筆 42731）──\n",
    "    base = base.drop_duplicates(subset=['ICUSTAY_ID'])\n",
    "\n",
    "    # 刪除非 AF 患者\n",
    "    base = base[base['HAS_AF'] == 1]\n",
    "\n",
    "    # (New) 只保留最後一筆 ICU 紀錄\n",
    "    print(\"只取每個病人最後一筆 ICU紀錄\")\n",
    "    # 先依 SUBJECT_ID 升序、INTIME 降序排序\n",
    "    base = base.sort_values(by=['SUBJECT_ID', 'INTIME'], ascending=[True, False])\n",
    "    # 對每個病人取第一筆（也就是最後一筆 ICU）\n",
    "    base = base.drop_duplicates(subset='SUBJECT_ID', keep='first')\n",
    "\n",
    "    # 儲存\n",
    "    base.to_csv(os.path.join(LOGS, \"base.csv\"), index=False)\n",
    "    return base\n",
    "\n",
    "base = load_clinical_tables()\n",
    "\n",
    "print(\"總資料: \",len(base))\n",
    "print(\"總共多少 subject id\",base['SUBJECT_ID'].nunique())\n",
    "\n",
    "\"\"\"Analysis with Final\"\"\"\n",
    "print(f\"Missing ID (total {len(total_set - set(base['SUBJECT_ID'].to_list()))}): {total_set - set(base['SUBJECT_ID'].to_list())}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e18749f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Exist Subject ID in MATCH/RECORD : 3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 12 id/s [00:19,  1.31 id/s/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 沒有符合規定的 header (包括結尾不含 n)\n",
      "507 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 21 id/s [00:33,  1.87s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 35 id/s [00:58,  2.17s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981 沒有符合規定的 header (包括結尾不含 n)\n",
      "1006 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 65 id/s [01:30,  1.47s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 92 id/s [02:52,  4.09s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2265 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 105 id/s [03:26,  3.75s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2619 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 254 id/s [11:14,  4.62s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6519 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 264 id/s [12:33,  6.06s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6659 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 268 id/s [12:34,  2.08s/ id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6749 沒有符合規定的 header (包括結尾不含 n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Subject id : : 312 id/s [15:34,  2.99s/ id/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 256\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m        last_header = select_latest_signal(wfdb_headers)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        last_match_ecg.append(last_header)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    match_record_df.to_csv(os.path.join(LOGS, \"ecg_match.csv\"),index=False)\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m#match_record_df=match_last_ecg_signal(set(base['SUBJECT_ID'].to_list()))\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m match_record_df\u001b[38;5;241m=\u001b[39m\u001b[43mmatch_last_ecg_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m match_record_df \u001b[38;5;241m=\u001b[39m match_record_df\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(total_set \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(match_record_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()))\n",
      "Cell \u001b[1;32mIn[27], line 220\u001b[0m, in \u001b[0;36mmatch_last_ecg_signal\u001b[1;34m(base)\u001b[0m\n\u001b[0;32m    217\u001b[0m         total_l2_sec\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     match_header \u001b[38;5;241m=\u001b[39m \u001b[43mselect_fitness_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSUBJECT_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSUBJECT_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwfdb_headers\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    221\u001b[0m     last_match_ecg\u001b[38;5;241m.\u001b[39mappend(match_header)\n\u001b[0;32m    222\u001b[0m match_record_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMATCH_ECG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m last_match_ecg\n",
      "Cell \u001b[1;32mIn[27], line 58\u001b[0m, in \u001b[0;36mselect_fitness_signal\u001b[1;34m(base_series, filenames)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn,dt \u001b[38;5;129;01min\u001b[39;00m parsed:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# print(f\"Current fn:{fn}\")\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     rec_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MATCH,fn[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m],fn\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m],fn\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hea\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 58\u001b[0m     hdr \u001b[38;5;241m=\u001b[39m \u001b[43mwfdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdheader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrd_segments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     base_dt \u001b[38;5;241m=\u001b[39m parse_base_datetime(hdr)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# ❶ 先算邊界\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wfdb\\io\\record.py:1915\u001b[0m, in \u001b[0;36mrdheader\u001b[1;34m(record_name, pn_dir, rd_segments)\u001b[0m\n\u001b[0;32m   1912\u001b[0m         record\u001b[38;5;241m.\u001b[39msegments\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1913\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1914\u001b[0m         record\u001b[38;5;241m.\u001b[39msegments\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m-> 1915\u001b[0m             \u001b[43mrdheader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpn_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1916\u001b[0m         )\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;66;03m# Fill in the sig_name attribute\u001b[39;00m\n\u001b[0;32m   1918\u001b[0m record\u001b[38;5;241m.\u001b[39msig_name \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mget_sig_name()\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wfdb\\io\\record.py:1855\u001b[0m, in \u001b[0;36mrdheader\u001b[1;34m(record_name, pn_dir, rd_segments)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;66;03m# If it isn't a cloud path or a PhysioNet path, we treat as a local file\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1854\u001b[0m     dir_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(dir_name)\n\u001b[1;32m-> 1855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[38;5;66;03m# Separate comment and non-comment lines\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\core.py:105\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\spec.py:1310\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1310\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\implementations\\local.py:201\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\implementations\\local.py:365\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\louislin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\implementations\\local.py:370\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    372\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================================= 根據 base.csv 結果篩選 WFDB ECG 訊號: ecg_match.csv =================================\n",
    "\n",
    "# Helper function\n",
    "def parse_base_datetime(hdr):\n",
    "    \"\"\"\n",
    "    處理 Header base time data type (適用於不同 wfdb 版本)\n",
    "    Args:\n",
    "    - hdr: wfdb.header 物件\n",
    "\n",
    "    Return:\n",
    "    - t0: 紀錄的起始時間, datetime 物件\n",
    "    \"\"\"\n",
    "    if getattr(hdr, \"base_datetime\", None):           # v4.x 直接提供\n",
    "        return hdr.base_datetime\n",
    "    # base_date 可能已是 datetime.date\n",
    "    bdate = hdr.base_date if isinstance(hdr.base_date, date) \\\n",
    "            else datetime.strptime(hdr.base_date, \"%d/%m/%Y\").date()\n",
    "    # base_time 可能已是 datetime.time\n",
    "    btime = hdr.base_time if isinstance(hdr.base_time, time) \\\n",
    "            else datetime.strptime(hdr.base_time.split(\".\")[0], \"%H:%M:%S\").time()\n",
    "    return datetime.combine(bdate, btime)\n",
    "\n",
    "def select_fitness_signal(base_series:pd.Series, filenames: List[str])->Optional[str]:\n",
    "    \"\"\"\n",
    "    篩選該筆 ICU 紀錄時間是否有吻合的 ECG 訊號\n",
    "    - Match Rule: ICUSTAY.INTIME in [base_datetime - 1hr , base_date_time + 1hr]\n",
    "\n",
    "    Args:\n",
    "    - base_series: pd.Series, including cols ['SUBJECT_ID','INTIME']\n",
    "    - filenames: List[str], match subject id 的所有 WFDB Header\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'-(\\d{4})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})\\.hea$')\n",
    "    parsed = []\n",
    "    \n",
    "    for fn in filenames:\n",
    "        m = pattern.search(fn)\n",
    "        if m:\n",
    "            dt = datetime.strptime(\n",
    "                \"-\".join(m.groups()[:3]) + \"-\" +\n",
    "                \":\".join(m.groups()[3:]),\n",
    "                \"%Y-%m-%d-%H:%M\"\n",
    "            )\n",
    "            parsed.append((fn, dt))\n",
    "        else:\n",
    "            print(f\"file {fn} 日期解析錯誤\")\n",
    "    \n",
    "    if not parsed:\n",
    "        print(f\"{filenames[0].split('-')[0]} : 沒有任何檔案被正確日期解析\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    intime = pd.to_datetime(base_series[\"INTIME\"]) + pd.Timedelta(minutes=30)\n",
    "    \n",
    "\n",
    "    for fn,dt in parsed:\n",
    "        # print(f\"Current fn:{fn}\")\n",
    "        rec_path = os.path.join(MATCH,fn[0:3],fn.split(\"-\")[0],fn.removesuffix(\".hea\"))\n",
    "        hdr = wfdb.rdheader(rec_path, rd_segments=True)\n",
    "        base_dt = parse_base_datetime(hdr)\n",
    "       \n",
    "        # ❶ 先算邊界\n",
    "        lower = base_dt - pd.Timedelta(hours=1)\n",
    "        upper = base_dt + pd.Timedelta(hours=1)\n",
    "        # ❷ 再做 AND 比對（左右都加括號）\n",
    "        if ((intime >= lower) & (intime <= upper)).any():\n",
    "            return fn  # 直接回傳檔名字串\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_latest_signal(filenames: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    擷取一連串 WFDB Header 檔案字串做時間處理，並篩選出最晚一筆的 Header\n",
    "    Args:\n",
    "    - filenames: List[str], match subject id 的所有 WFDB Header\n",
    "\n",
    "    Return:\n",
    "    - last_header: str, WFDB 的最後一筆 Header 檔案名稱\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'-(\\d{4})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})\\.hea$')\n",
    "    parsed = []\n",
    "    \n",
    "    for fn in filenames:\n",
    "        m = pattern.search(fn)\n",
    "        if m:\n",
    "            dt = datetime.strptime(\n",
    "                \"-\".join(m.groups()[:3]) + \"-\" +\n",
    "                \":\".join(m.groups()[3:]),\n",
    "                \"%Y-%m-%d-%H:%M\"\n",
    "            )\n",
    "            parsed.append((fn, dt))\n",
    "        else:\n",
    "            print(f\"file {fn} 日期解析錯誤\")\n",
    "    \n",
    "    if not parsed:\n",
    "        print(f\"{filenames[0].split('-')[0]} : 沒有任何檔案被正確日期解析\")\n",
    "        return None\n",
    "    return max(parsed, key=lambda x: x[1])[0]\n",
    "\n",
    "def last_lead2_segment_end(hdr, rec_dir: str) -> Optional[Tuple[datetime, float]]:\n",
    "    \"\"\"\n",
    "    計算每一個 含有 Lead II segment 的 ECG 時間長度(in sec)，並以最後一個含 Lead II 的 segment 的結束作為整段ECG訊號的結束(即 T1)\n",
    "\n",
    "    Args:\n",
    "    - hdr: wfdb.rdheader() 回傳的 MultiRecordHeader 物件\n",
    "    - rec_dir: 該 header 檔案所在資料夾 (Path，不含 .hea)\n",
    "\n",
    "    Return\n",
    "    - (T1, total_lead2_sec): \n",
    "        T1 為最後一個 Lead II segment 的結束 datetime\n",
    "        total_lead2_sec 為所有 segment 中包含 Lead II 的總秒數\n",
    "      若完全沒有 Lead II，回傳 None。\n",
    "    \"\"\"\n",
    "    base_dt = parse_base_datetime(hdr)\n",
    "    if base_dt is None:\n",
    "        return None\n",
    "\n",
    "    # 1) 取段名與長度（nsamp） —— 來源 hdr.seg_name / hdr.seg_len\n",
    "    seg_names = hdr.seg_name           # list[str]\n",
    "    seg_lens  = hdr.seg_len            # list[int]  :contentReference[oaicite:3]{index=3}\n",
    "    assert len(seg_names) == len(seg_lens)\n",
    "\n",
    "    # 2) 預先算 cumulative start index\n",
    "    cum = [0]\n",
    "    for L in seg_lens[:-1]:\n",
    "        cum.append(cum[-1] + L)\n",
    "\n",
    "    total_sec = 0.0\n",
    "    last_end  = None\n",
    "\n",
    "    # 3) 倒序掃描段\n",
    "    for seg_name, start_idx, seg_len in reversed(list(zip(seg_names, cum, seg_lens))):\n",
    "        if seg_name == \"~\":            # gap 段 :contentReference[oaicite:4]{index=4}\n",
    "            continue\n",
    "        sub_hdr = wfdb.rdheader(Path(rec_dir, seg_name))\n",
    "        if \"II\" not in [n.upper().replace(\" \", \"\") for n in sub_hdr.sig_name]:\n",
    "            continue\n",
    "\n",
    "        seg_sec = seg_len / sub_hdr.fs\n",
    "        total_sec += seg_sec\n",
    "        \n",
    "        if last_end is None: #找到最後一段，先行紀錄\n",
    "            seg_start_abs = base_dt + timedelta(seconds=start_idx / hdr.fs)\n",
    "            last_end = seg_start_abs + timedelta(seconds=seg_sec)\n",
    "\n",
    "\n",
    "    if last_end is None:\n",
    "        return None\n",
    "    return last_end, total_sec\n",
    "\n",
    "\n",
    "def header_time_range(header_path:str)->Optional[Tuple[datetime,datetime,float]]:\n",
    "    \"\"\"\n",
    "    計算 ECG訊號的起始時間，關於時間算法的界定如下:\n",
    "    - T0: ECG 訊號起始時間，這裡採用 `header.base_dt`: 整個 multi-segment record 的起始時間，而不是某個 segment 的起始時間\n",
    "        - 更精確做法應該採用 seg 內段落(待考慮，予以保留)\n",
    "    - T1: ECG 訊號結束時間\n",
    "        - 採用最後一筆 LEAD II seg 作為ECG訊號截止時間\n",
    "        - 計算方式: \n",
    "            - seg_end = seg_start + (seg_length_in_seconds)\n",
    "            - seg_length_in_seconds = sub_hdr.sig_len / sub_hdr.fs\n",
    "    Arg:\n",
    "    - header_path: WFDB header 的完整path : Z:/p00/p000085/p000085-2167-07-25-21-11.hea\n",
    "    \"\"\"\n",
    "    rec_path   = Path(header_path).with_suffix(\"\")    # 去掉 .hea\n",
    "    hdr        = wfdb.rdheader(str(rec_path), rd_segments=True)\n",
    "    if hdr.base_time is None or hdr.base_date is None:\n",
    "        print(f\"{rec_path.name}: missing base_date/time\")\n",
    "        return None\n",
    "\n",
    "    t0 = parse_base_datetime(hdr)\n",
    "    res = last_lead2_segment_end(hdr, rec_path.parent)\n",
    "    if res is None:\n",
    "        print(f\"{rec_path.name}: 不存在 Lead II\")\n",
    "        return None\n",
    "\n",
    "    t1, l2_sec = res\n",
    "    return t0, t1, l2_sec\n",
    "\n",
    "# main preprocsee pipeline\n",
    "def match_last_ecg_signal(base:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    根據 subject id 搜尋是否有對應的 ECG 訊號，如果單一 subject id 對應多段 ECG singal(WFDB Header) 則只讀取最後一段\n",
    "\n",
    "    Args:\n",
    "    - base_subject_id: Set, base 篩選出來的 subject_id\n",
    "    \"\"\"\n",
    "    record_df = pd.read_csv(os.path.join(DATA_CSV,\"RECORDS.csv\"),dtype={\n",
    "        \"SUBJECT_ID\":int,\n",
    "        \"PATH\":str\n",
    "    })\n",
    "    match_record_df = record_df[record_df['SUBJECT_ID'].isin(base['SUBJECT_ID'])]\n",
    "    print(f\"Total Exist Subject ID in MATCH/RECORD : {len(match_record_df)}\")\n",
    "\n",
    "    last_match_ecg = []\n",
    "    last_t0 = []\n",
    "    last_t1 = []\n",
    "    total_l2_sec = []\n",
    "\n",
    "    # 篩選 match 過後的 record 並找尋最後一筆 ECG Header\n",
    "\n",
    "    for _, row in tqdm(match_record_df.iterrows(), desc=\"Total Subject id : \", unit=\" id/s\"):\n",
    "        rec_paths = os.path.join(MATCH, row['PATH']) # rec_paths: Z://pxx//pxxnnnn\n",
    "        \n",
    "        \n",
    "        wfdb_headers = [x for x in os.listdir(rec_paths) if row['PATH'].split(\"/\")[1] in x and not x.split(\".\")[0].endswith(\"n\")]\n",
    "        if not wfdb_headers:\n",
    "            print(f\"{row['SUBJECT_ID']} 沒有符合規定的 header (包括結尾不含 n)\")\n",
    "            record_log(row['SUBJECT_ID'],\"沒有符合規定的 header (包括結尾不含 n)\")\n",
    "            last_match_ecg.append(None)\n",
    "            last_t0.append(None)\n",
    "            last_t1.append(None)\n",
    "            total_l2_sec.append(0)\n",
    "            continue\n",
    "\n",
    "        match_header = select_fitness_signal(base.loc[base['SUBJECT_ID']==row[\"SUBJECT_ID\"]],wfdb_headers) \n",
    "        last_match_ecg.append(match_header)\n",
    "    match_record_df['MATCH_ECG'] = last_match_ecg\n",
    "    match_record_df.to_csv(os.path.join(LOGS, \"test_ecg_match.csv\"),index=False)\n",
    "    return match_record_df\n",
    "    \"\"\"\n",
    "        last_header = select_latest_signal(wfdb_headers)\n",
    "        last_match_ecg.append(last_header)\n",
    "\n",
    "        # 計算 T0/T1/Total Lead II secs\n",
    "        result = None  # 初始化，避免未定義錯誤\n",
    "\n",
    "        if last_header:\n",
    "            result = header_time_range(Path(os.path.join(rec_paths,last_header)))\n",
    "        else:\n",
    "            record_log(row['SUBJECT_ID'],\"沒有找到最後一筆的 Header\")\n",
    "\n",
    "        if result:\n",
    "            last_t0.append(result[0])\n",
    "            last_t1.append(result[1])\n",
    "            total_l2_sec.append(result[2])\n",
    "        else:\n",
    "            last_t0.append(None)\n",
    "            last_t1.append(None)\n",
    "            total_l2_sec.append(0)\n",
    "            record_log(row['SUBJECT_ID'],\"所有的 segment record 都沒有 LEAD II ECG\")\n",
    "        \n",
    "    match_record_df[\"LAST_ECG\"] = last_match_ecg\n",
    "    match_record_df['T0'] = last_t0\n",
    "    match_record_df[\"T1\"] = last_t1\n",
    "    match_record_df['LEAD2_SEC'] = total_l2_sec\n",
    "\n",
    "    match_record_df.to_csv(os.path.join(LOGS, \"ecg_match.csv\"),index=False)\n",
    "    \"\"\"\n",
    "\n",
    "#match_record_df=match_last_ecg_signal(set(base['SUBJECT_ID'].to_list()))\n",
    "match_record_df=match_last_ecg_signal(base)\n",
    "match_record_df = match_record_df.dropna(axis=0)\n",
    "print(total_set - set(match_record_df['SUBJECT_ID'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df7c978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ECG Record : 6632\n",
      "After Matching, total data : 2238, Total Subject ID: 2238\n",
      "set()\n",
      "Yuran Alive Missing (total: 0) : \n",
      "set()\n",
      "Yuran dead Missing (total: 0) : \n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# ================================= 篩選 ECG 時間段與匹配 Clinical Data : merge_subject_filtered/detail.csv =================================\n",
    "#helper function\n",
    "# 多補上 ECG 時間需要落在 入出院時間\n",
    "def ecg_in_icu(base_row:pd.Series, ecg_match:pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    判斷 ECG record 時間是否在ICU離開時間:\n",
    "    Yuran Rules\n",
    "    - 不檢查 ECG 起點是否大於 ADMITTIME (預設 ECG 一定會落在內)\n",
    "    - Range formula: 只要有任一筆 T1 ≤ OUTTIME + 30 分鐘，回傳 True\n",
    "    Args:\n",
    "    - base_row : pandas.Series\n",
    "        必須含下列欄位：\n",
    "        'SUBJECT_ID' : int\n",
    "        'INTIME'  : str 或 datetime  (ICU 進入時間，可不使用)\n",
    "        'OUTTIME'  : str 或 datetime  (ICU 離開時間)\n",
    "    - ecg_match : pandas.DataFrame\n",
    "        欄位必須包含：\n",
    "        ['SUBJECT_ID', 'T0', 'T1']\n",
    "\n",
    "    Returns\n",
    "    - bool\n",
    "        - True  → 至少有一筆 ECG 結束時間 T1 在（OUTTIME + 30 分鐘）之前\n",
    "        - False → 所有 ECG 訊號都超出此時間，或根本沒有 ECG 記錄（並已記錄 log）\n",
    "    \"\"\"\n",
    "    # --- 1. 解析ICU期間 --------------------------------------\n",
    "    sid        = int(base_row[\"SUBJECT_ID\"])\n",
    "    icuout_time = pd.to_datetime(base_row[\"OUTTIME\"]) + pd.Timedelta(minutes=30)\n",
    "\n",
    "\n",
    "    # --- 2. 擷取該病人所有 ECG 記錄 --------------------------\n",
    "    ecg = ecg_match[ecg_match.SUBJECT_ID == sid]\n",
    "    if ecg.empty:\n",
    "        record_log(sid,\"在檢查 ecg_in_icu 時沒有 ECG signal\")\n",
    "        return False   # 此病人沒有波形\n",
    "\n",
    "    if ecg[\"T0\"].dtype == \"object\":\n",
    "        ecg = ecg.assign(T0=pd.to_datetime(ecg[\"T0\"]),\n",
    "                         T1=pd.to_datetime(ecg[\"T1\"]))\n",
    "        \n",
    "    # --- 3. 檢查是否處於 ICU 離院前 ------------------------------\n",
    "    # 條件：  (T1 <= DISCHTIME)\n",
    "    in_window = (ecg[\"T1\"] <= icuout_time).any()\n",
    "\n",
    "    return bool(in_window)\n",
    "\n",
    "\n",
    "def ecg_in_admission(base_row:pd.Series, ecg_match:pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    判斷 ECG record 時間是否完全落於該次入院/出院時間:\n",
    "    \n",
    "    - Range formula: [T0, T1] ⊆ [ADMITTIME-30min, DISCHTIME] \n",
    "    \n",
    "    Args:\n",
    "    - base_row : pandas.Series\n",
    "        必須含下列欄位：\n",
    "        'SUBJECT_ID' : int\n",
    "        'ADMITTIME'  : str 或 datetime  (住院 進入時間)\n",
    "        'DISCHTIME'  : str 或 datetime  (出院 離開時間)\n",
    "    - match_ecg_row : pandas.DataFrame\n",
    "        欄位必須包含：\n",
    "        ['SUBJECT_ID', 'T0', 'T1']\n",
    "\n",
    "    Returns\n",
    "    - bool\n",
    "        True  → ECG 訊號落在允許出入院期間\n",
    "        False → 完全無交集\n",
    "    \"\"\"\n",
    "    # --- 1. 解析住院期間 --------------------------------------\n",
    "    sid        = int(base_row[\"SUBJECT_ID\"])\n",
    "    admit_time = pd.to_datetime(base_row[\"ADMITTIME\"]) - pd.Timedelta(minutes=30)\n",
    "    disch_time = pd.to_datetime(base_row[\"DISCHTIME\"])\n",
    "\n",
    "    # --- 2. 擷取該病人所有 ECG 記錄 --------------------------\n",
    "    ecg = ecg_match[ecg_match.SUBJECT_ID == sid]\n",
    "    if ecg.empty:\n",
    "        return False   # 此病人沒有波形\n",
    "\n",
    "    if ecg[\"T0\"].dtype == \"object\":\n",
    "        ecg = ecg.assign(T0=pd.to_datetime(ecg[\"T0\"]),\n",
    "                         T1=pd.to_datetime(ecg[\"T1\"]))\n",
    "    # --- 3. 檢查是否「完整包含」 ------------------------------\n",
    "    # 條件： (T0 >= ADMITTIME-30min) AND (T1 <= DISCHTIME)\n",
    "    in_window = (\n",
    "        (ecg[\"T0\"] >= admit_time) &\n",
    "        (ecg[\"T1\"] <= disch_time)\n",
    "    ).any()\n",
    "\n",
    "    return bool(in_window)\n",
    "\n",
    "def ecg_overlaps_icu(base_row:pd.Series, ecg_match:pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    判斷單一 ICU 住院是否有任何 ECG record 與 ICU 期間重疊\n",
    "    \n",
    "    Args:\n",
    "    - base_row : pandas.Series\n",
    "        必須含下列欄位：\n",
    "        'SUBJECT_ID' : int\n",
    "        'INTIME'     : str 或 datetime  (ICU 進入時間)\n",
    "        'OUTTIME'    : str 或 datetime  (ICU 離開時間)\n",
    "        'ADMITTIME'  : str 或 datetime  (住院 進入時間)\n",
    "        'DISCHTIME'  : str 或 datetime  (出院 離開時間)\n",
    "    - match_ecg_row : pandas.DataFrame\n",
    "        欄位必須包含：\n",
    "        ['SUBJECT_ID', 'T0', 'T1']\n",
    "\n",
    "    Returns\n",
    "    - bool\n",
    "        True  → ICU 期間與任一 ECG (T0,T1) 有交集\n",
    "        False → 完全無交集\n",
    "    \"\"\"\n",
    "    # --- 1. 解析 ICU 期間 --------------------------------------\n",
    "    sid   = int(base_row['SUBJECT_ID'])\n",
    "    intime  = pd.to_datetime(base_row['INTIME'])\n",
    "    outtime = pd.to_datetime(base_row['OUTTIME'])\n",
    "\n",
    "    # --- 2. 擷取該病人的所有 ECG 記錄 --------------------------\n",
    "    ecg = ecg_match[ecg_match.SUBJECT_ID == sid]\n",
    "    if ecg.empty:\n",
    "        return False   # 無任何波形\n",
    "    \n",
    "    if ecg[\"T0\"].dtype == \"object\":\n",
    "        ecg = ecg.assign(T0=pd.to_datetime(ecg[\"T0\"]),\n",
    "                         T1=pd.to_datetime(ecg[\"T1\"]))\n",
    "\n",
    "    # --- 3. 檢查是否重疊 --------------------------------------\n",
    "    # 條件： max(T0, INTIME) < min(T1, OUTTIME)\n",
    "    overlaps = (\n",
    "        (ecg['T0'].clip(lower=intime) <\n",
    "         ecg['T1'].clip(upper=outtime))\n",
    "        .any()\n",
    "    )\n",
    "    return bool(overlaps)\n",
    "# main processing pipeline\n",
    "def match_ecg_clinic(base:pd.DataFrame, ecg_match:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    匹配 ECG 訊號屬於 SUBJECT ID 中哪一次的 HADM_ID -> ICUSTAY_ID, 先以 T0,T1 有沒有與ICU.INTIME/OUTTIME Overlap\n",
    "    \"\"\"\n",
    "    ecg_match_drop = ecg_match.dropna()\n",
    "    print(f\"Total ECG Record : {len(ecg_match_drop)}\")\n",
    "    merge_subject_detail = base.copy()\n",
    "    merge_subject_detail['ECG_OVERLAP'] = base.apply(ecg_overlaps_icu, axis=1, ecg_match =ecg_match_drop )\n",
    "    merge_subject_filterd = merge_subject_detail[merge_subject_detail['ECG_OVERLAP']]\n",
    "\n",
    "    print(f\"After Matching, total data : {len(merge_subject_filterd)}, Total Subject ID: {merge_subject_filterd['SUBJECT_ID'].nunique()}\")\n",
    "    merge_subject_filterd.to_csv(os.path.join(LOGS,\"merge_test.csv\"),index=False)\n",
    "    return merge_subject_filterd\n",
    "\n",
    "def merge_ecg_clinic(base:pd.DataFrame, ecg_match:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - 匹配 ECG 訊號屬於 SUBJECT ID 中哪一次的 HADM_ID -> ICUSTAY_ID\n",
    "        - 以時間段匹配，ECG的 T0,T1必須介於該次出入院內\n",
    "    - 流程\n",
    "        - 移除 `ecg_match` 中不存在T0,T1的資料(保留方便複查)\n",
    "        - 匹配 ICU_STAY_TIME 與 T0,T1 是否有 Overlap\n",
    "        - 移除不匹配的紀錄\n",
    "        - 進一步檢查 T0有沒有\n",
    "    \n",
    "    Args:\n",
    "    - base: pd.Dataframe, made from function load_clinical_tables()\n",
    "    - ecg_match: pd.Dataframe, made from function match_last_ecg_signal()\n",
    "\n",
    "    Return:\n",
    "    \"\"\"\n",
    "    ecg_match_drop = ecg_match.dropna()\n",
    "    print(f\"Total ECG Record : {len(ecg_match_drop)}\")\n",
    "\n",
    "    merge_subject_detail = base.copy()\n",
    "    merge_subject_detail['ECG_OVERLAP'] = base.apply(ecg_overlaps_icu, axis=1, ecg_match =ecg_match_drop )\n",
    "\n",
    "    # 改用嚴格匹配 ICU 離開時間: ecg_in_admission -> replace with ecg_in_icu\n",
    "    # merge_subject_detail['ECG_IN_ADMISSION'] = base.apply(ecg_in_admission, axis=1, ecg_match =ecg_match_drop)\n",
    "    merge_subject_detail['ECG_IN_ICU'] = base.apply(ecg_in_icu, axis=1, ecg_match =ecg_match_drop)\n",
    "\n",
    "    merge_subject_detail.to_csv(os.path.join(LOGS,\"merge_subject_detail.csv\"),index= False)\n",
    "\n",
    "    merge_subject_filterd = merge_subject_detail[merge_subject_detail['ECG_IN_ICU'] & merge_subject_detail['ECG_OVERLAP']]\n",
    "    merge_subject_filterd.to_csv(os.path.join(LOGS,\"merge_subject_filtered.csv\"),index= False)\n",
    "\n",
    "    # 多筆 subject id 通過條件分析\n",
    "    counts = merge_subject_filterd[\"SUBJECT_ID\"].value_counts()\n",
    "    multi = counts[counts > 1].index.tolist()\n",
    "    print(\"同一個 subject_id 出現超過一次：\", multi)\n",
    "    \n",
    "    return merge_subject_filterd\n",
    "\n",
    "ecg_match = pd.read_csv(os.path.join(LOGS,\"ecg_match_info.csv\"))\n",
    "# merge_subject_filterd = merge_ecg_clinic(base,ecg_match)\n",
    "\n",
    "\"\"\"先進行 Overlap 分析結果\"\"\"\n",
    "merge_subject_filterd = match_ecg_clinic(base,ecg_match)\n",
    "alive_set = set(alive_yuran['SUBJECT_ID'].to_list())\n",
    "dead_set =  set(dead_yuran['SUBJECT_ID'].to_list())\n",
    "print(alive_set&dead_set)\n",
    "\n",
    "total_subject_yuran = set(alive_yuran['SUBJECT_ID'].to_list()) | set(dead_yuran['SUBJECT_ID'].to_list())\n",
    "print(f\"Yuran Alive Missing (total: {len(alive_set-set(merge_subject_filterd['SUBJECT_ID'].to_list()))}) : \\n{alive_set-set(merge_subject_filterd['SUBJECT_ID'].to_list())}\")\n",
    "print(f\"Yuran dead Missing (total: {len(dead_set-set(merge_subject_filterd['SUBJECT_ID'].to_list()))}) : \\n{dead_set-set(merge_subject_filterd['SUBJECT_ID'].to_list())}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e9cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========  Stage 1 Result ==============\n",
      "Survior Cohort: Number of Data 1775; Total Subject ID: 1775\n",
      "Deceased Cohort: Number of Data 398; Total Subject ID: 398\n",
      "✅ SUBJECT_ID 存在於 My Surv 但不存在於 Yuran Surv, 共 1444:\n",
      "{'73190', '10241', '83272', '63512', '24925', '61956', '21071', '84775', '17412', '79352', '71652', '41882', '32247', '4490', '46467', '69776', '93602', '18910', '51642', '65370', '25452', '15583', '75034', '15079', '59102', '81349', '50643', '24693', '76844', '66311', '95948', '18614', '89459', '84150', '14298', '20795', '53355', '43738', '9486', '3695', '15610', '58022', '93479', '69871', '90814', '98961', '57511', '98555', '1528', '9274', '93159', '94768', '6116', '60272', '26380', '76801', '19125', '18254', '71596', '63525', '76797', '72999', '67087', '43296', '18837', '65404', '15538', '54675', '89556', '84206', '41204', '29664', '63961', '89012', '23510', '99560', '98494', '85', '15198', '14532', '63733', '54174', '59225', '29215', '98280', '42721', '72151', '64137', '86381', '80286', '15021', '75607', '31633', '2442', '65925', '13948', '40179', '94164', '30582', '45199', '64336', '60798', '27661', '81212', '79922', '86411', '4520', '54153', '22585', '68956', '9178', '8363', '95294', '87428', '29581', '6070', '23440', '3174', '45300', '4420', '74851', '97565', '90649', '94581', '93422', '21769', '10928', '79589', '20860', '22557', '50537', '52816', '15619', '68915', '29712', '56986', '62447', '47233', '76459', '42663', '4431', '16888', '96732', '57905', '6335', '54397', '95688', '63621', '26381', '56947', '50744', '53020', '72992', '99102', '98769', '21965', '63107', '50237', '97013', '94301', '80210', '4338', '44969', '78214', '62032', '57815', '8281', '87992', '44570', '92244', '31470', '75155', '57097', '99064', '16776', '96225', '44908', '59833', '47715', '71857', '83300', '88407', '73409', '61149', '46109', '95821', '56391', '85389', '31057', '24856', '78441', '21115', '29343', '63074', '8396', '12267', '93301', '57489', '87203', '20984', '88343', '69082', '92764', '67418', '44369', '50772', '3351', '70974', '59161', '71328', '54736', '66807', '17497', '9732', '74223', '73868', '64717', '9311', '76196', '96261', '21321', '59135', '44298', '46156', '28386', '1313', '56996', '8532', '16210', '47634', '90522', '82257', '74546', '2029', '51082', '83751', '90990', '22495', '43729', '51301', '12482', '7253', '7968', '79081', '72891', '68605', '77227', '69522', '95754', '69591', '59198', '72779', '70329', '90690', '99796', '26105', '93722', '50113', '55901', '69162', '55611', '95542', '24142', '68075', '63750', '83981', '60053', '18711', '46373', '59215', '11855', '78939', '49955', '60343', '19872', '31973', '18123', '47709', '63669', '96791', '17083', '11870', '75582', '12400', '82928', '75160', '72358', '63582', '1396', '18952', '70494', '47203', '69371', '22289', '19624', '29509', '17394', '15637', '22782', '75029', '88003', '15470', '75793', '17753', '23599', '16122', '78238', '51017', '50089', '99088', '21786', '78979', '10581', '40624', '56697', '19016', '44468', '73460', '46817', '87239', '74346', '46287', '7533', '64798', '60389', '88013', '3886', '83887', '55473', '65425', '56364', '31279', '54600', '17795', '56449', '12727', '84633', '56849', '79032', '16709', '84749', '61441', '12508', '89265', '61619', '77135', '43705', '85286', '81063', '4807', '43926', '22466', '44789', '18498', '26109', '49015', '61569', '50041', '67239', '87936', '86245', '4968', '7470', '13615', '88269', '70957', '79330', '43323', '96015', '69272', '6967', '86193', '17803', '22393', '85235', '64112', '30669', '19296', '61980', '46080', '75772', '53136', '53176', '95854', '63290', '1158', '3214', '30054', '24711', '10042', '52897', '2996', '22954', '16013', '4833', '98957', '41881', '25117', '222', '65052', '11171', '3992', '88734', '29511', '23666', '48037', '31034', '16839', '89179', '12110', '85757', '16992', '89528', '1931', '6063', '22432', '85493', '58270', '99216', '18786', '40460', '41758', '55143', '14914', '87789', '62603', '57678', '79283', '81491', '94669', '98169', '2549', '79602', '72160', '19342', '76930', '22418', '58508', '92698', '62415', '28897', '50832', '56802', '29829', '65537', '32511', '865', '52696', '72467', '86086', '11043', '24556', '70451', '8105', '67589', '93721', '2754', '28698', '73537', '21860', '86719', '93432', '16139', '89565', '90917', '50880', '83908', '66903', '66473', '44666', '59701', '17629', '74805', '68242', '48674', '10257', '50385', '41389', '60852', '87608', '50885', '5307', '70854', '6561', '27823', '19297', '87754', '26031', '94896', '53342', '50915', '74463', '62721', '85071', '75972', '14780', '43033', '69052', '89698', '67477', '11850', '99666', '18921', '41194', '60057', '99674', '89714', '42035', '1485', '19087', '71774', '92886', '45838', '798', '3744', '1949', '6039', '53724', '44784', '7825', '41078', '9987', '51466', '29073', '49635', '75509', '84544', '76726', '58303', '13694', '89772', '13759', '53875', '52109', '63131', '94726', '97028', '78691', '61207', '4136', '23132', '70500', '44044', '90834', '82847', '52355', '69709', '18733', '14245', '42311', '45801', '67377', '9991', '88696', '19411', '25115', '78050', '45936', '25882', '6279', '84392', '10152', '13071', '94840', '21683', '92649', '60309', '91625', '77826', '21507', '28684', '20742', '43798', '52766', '10534', '15382', '50649', '67711', '68401', '29120', '50793', '86692', '80142', '31648', '26781', '27060', '99776', '63003', '95609', '58899', '58247', '68909', '49499', '92052', '88790', '62717', '27540', '11431', '50729', '55722', '68676', '65779', '3920', '3512', '48688', '91169', '46237', '46449', '10247', '2589', '2157', '51936', '22766', '99611', '59960', '65753', '23413', '6365', '21710', '71513', '87975', '54429', '1072', '69563', '29199', '64374', '26594', '47309', '27463', '96965', '946', '81866', '3570', '214', '99645', '43866', '52952', '31051', '2395', '54735', '70273', '75355', '23085', '93847', '78565', '48910', '58113', '71491', '13052', '94645', '55849', '4538', '20324', '65013', '708', '9664', '16129', '61030', '59457', '66822', '9332', '28869', '85393', '58732', '63771', '11728', '62884', '78010', '75581', '15243', '52647', '11147', '21975', '81536', '40797', '72671', '45816', '46776', '70016', '90389', '73530', '56201', '52762', '71296', '64145', '1170', '46041', '63701', '4369', '44408', '57023', '21561', '60969', '66067', '11191', '2744', '69293', '6090', '68797', '79480', '98991', '93411', '49118', '5126', '98344', '15687', '95474', '67348', '23321', '99616', '22104', '43060', '23627', '4180', '15298', '92903', '60829', '15567', '30733', '26136', '75333', '99883', '12113', '66919', '26494', '89734', '62254', '13218', '77452', '82694', '25729', '58310', '60735', '60034', '81893', '86041', '77301', '97448', '1818', '43472', '26978', '25016', '9993', '92846', '43155', '29270', '41619', '12351', '52199', '66288', '10924', '72790', '7842', '91365', '30486', '61012', '18815', '75779', '25297', '90354', '1459', '19604', '16032', '14005', '24923', '14702', '18248', '25317', '32288', '3272', '28785', '75071', '71532', '81807', '27282', '70026', '11003', '54958', '58793', '48006', '5259', '22642', '4266', '95282', '48397', '44326', '48239', '15885', '32442', '83932', '12903', '9676', '22384', '71952', '21771', '59546', '77873', '67415', '86318', '72459', '91946', '23038', '16024', '65448', '41217', '1941', '69681', '69011', '19145', '92525', '51660', '64361', '29826', '82159', '61835', '80030', '7225', '44061', '92136', '52039', '94195', '25168', '21630', '69108', '95420', '21108', '22364', '60949', '11638', '23034', '4788', '32085', '51377', '46527', '22039', '82765', '9016', '5525', '73118', '15563', '22731', '60739', '58242', '88851', '26568', '6078', '52592', '53876', '18413', '25621', '23577', '59469', '72439', '71871', '58377', '9714', '90269', '41254', '1038', '23652', '47093', '41902', '74493', '7654', '31684', '12217', '24461', '29946', '16279', '63237', '63563', '14551', '44976', '90020', '98517', '47749', '32447', '74503', '96574', '74610', '66654', '46816', '638', '21734', '77951', '32193', '84505', '217', '99186', '18998', '28808', '78226', '98878', '55601', '15749', '63116', '80046', '52550', '85171', '7400', '26467', '50976', '97917', '52899', '85163', '74779', '96445', '61519', '1144', '31585', '7192', '58128', '52802', '92464', '75960', '89906', '81875', '262', '13432', '72571', '44128', '32067', '54147', '99291', '47660', '2104', '2514', '95237', '69265', '2981', '69693', '77413', '49739', '18568', '97232', '14626', '46797', '51942', '97984', '98959', '32474', '69675', '82010', '58938', '19666', '91853', '23105', '19634', '84776', '45979', '3866', '17976', '44058', '84478', '4018', '82565', '48145', '24063', '17152', '47216', '47546', '83288', '76544', '9967', '98720', '16499', '82681', '32671', '6889', '75771', '46984', '695', '60130', '5114', '18357', '11512', '20919', '79163', '94297', '17280', '1474', '25332', '22225', '76520', '92287', '80880', '51045', '605', '26377', '82898', '91579', '93784', '87817', '12094', '81216', '19981', '15185', '48076', '23580', '94821', '43975', '19405', '27148', '19208', '13422', '44742', '91200', '86589', '20704', '88747', '76557', '19765', '81203', '27456', '81295', '96321', '7996', '78100', '15821', '89818', '11698', '29336', '66786', '2636', '8432', '81408', '82685', '1563', '61656', '95603', '51064', '90972', '98565', '64334', '95182', '63028', '84886', '1418', '79998', '28077', '78336', '31290', '1012', '98434', '89148', '93528', '59788', '8231', '32373', '48212', '57964', '94256', '91915', '10423', '32082', '66692', '86561', '10525', '5071', '93272', '62715', '66717', '96592', '99759', '30101', '56460', '53290', '68916', '16164', '60115', '97786', '78221', '8896', '64082', '30410', '54523', '62919', '14777', '24609', '88976', '28660', '93648', '76096', '10342', '10667', '17617', '58631', '30026', '47718', '65703', '55477', '74639', '53282', '28753', '92415', '93638', '29477', '42525', '75347', '50110', '82585', '98973', '14458', '43759', '68065', '64601', '26734', '70740', '42702', '22242', '46063', '16256', '43206', '45477', '9021', '49685', '60680', '46471', '83584', '23474', '98733', '57445', '80885', '17236', '21025', '16399', '71230', '17218', '40236', '40334', '99120', '42130', '3935', '80154', '2480', '42310', '82393', '1898', '1449', '7410', '42360', '67684', '63447', '96697', '28065', '99011', '23097', '23380', '86355', '21447', '28941', '72592', '15569', '99830', '52791', '48640', '53612', '6892', '735', '10030', '5193', '3746', '77520', '48701', '42093', '17105', '44532', '94993', '59801', '83857', '45914', '77578', '28294', '60424', '56443', '69047', '22664', '5686', '48780', '11641', '70520', '57208', '41733', '13183', '40967', '65247', '79584', '66530', '99545', '20620', '90533', '49879', '12663', '81229', '23061', '22904', '75083', '80675', '52934', '95849', '93667', '30606', '47473', '7172', '3675', '99412', '45088', '72097', '6229', '8070', '88982', '96594', '15279', '56378', '28416', '91031', '71262', '46264', '43233', '47827', '32380', '27927', '2467', '93378', '32628', '45495', '55616', '8274', '77686', '17300', '70319', '98643', '86585', '71872', '55027', '49976', '1900', '24123', '65732', '3386', '7213', '66365', '94361', '23893', '22774', '56772', '54195', '12821', '93208', '91143', '2090', '50703', '46926', '89502', '41958', '16196', '21910', '3365', '40689', '91635', '61620', '73615', '94103', '57314', '99564', '99982', '1840', '18524', '21187', '52363', '17423', '99714', '23015', '3515', '27961', '44633', '61658', '86516', '5196', '48666', '99389', '30436', '96945', '80313', '16810', '91669', '90410', '95090', '21857', '16088', '11945', '65263', '92203', '31191', '76709', '81041', '1121', '49380', '91309', '22383', '44763', '12461', '20794', '47406', '22616', '76800', '10320', '55992', '64621', '48390', '76058', '87275', '96361', '23594', '32421', '91463', '51072', '98016', '2213', '47724', '66859', '11161', '48058', '96482', '91726', '46608', '90843', '59462', '78431', '11752', '32210', '76193', '10774', '86662', '19999', '849', '26661', '51986', '16821', '43037', '75086', '41163', '24532', '19375', '5850', '80586', '81885', '52462', '86757', '67149', '48637', '16019', '72017', '16337', '22918', '28039', '47288', '90676', '47444', '44454', '46502', '23459', '43827', '32125', '29463', '73540', '64280', '57157', '45064', '46910', '77807', '27691', '4802', '53541', '85352', '97834', '54073', '46339', '51121', '4059', '82179', '6053', '42444', '66831', '29961', '58205', '93966', '7849', '52566', '17616', '99922', '58993', '77975', '90158', '16504', '82466', '75826', '93117', '23292', '68126', '12831', '52778', '22304', '98249', '66189', '23584', '50141', '16961', '92650', '78410', '20013', '15701', '22565', '89742', '44521', '46197', '40736', '1973', '80423', '49255', '64557', '6673', '70822', '82831', '32084', '68724', '12565', '25949', '45866', '28044', '54609', '56333', '97529', '91802', '14079', '64994', '47892', '92613', '30484', '60118', '54221', '4406', '94838', '95782'}\n",
      "\n",
      "✅ SUBJECT_ID 存在於 Yuran Surv 但不存在於 My Surv共 35:\n",
      "{'88921', '5727', '29730', '83782', '78892', '52746', '67429', '82843', '92235', '84874', '12365', '77524', '19866', '41976', '94785', '5685', '91181', '17667', '80106', '86209', '40569', '47677', '62035', '54935', '26055', '94853', '30170', '44437', '53102', '82000', '42510', '85551', '45608', '60641', '63486'}\n",
      "✅ SUBJECT_ID 存在於 My Dead 但不存在於 Yuran Dead, 共 301:\n",
      "{'40548', '62782', '2063', '22134', '29466', '53856', '14094', '13436', '65634', '50353', '2332', '41446', '91350', '59726', '25131', '68391', '45601', '21152', '77691', '5642', '69512', '32582', '15461', '48827', '3242', '91549', '76876', '10305', '90228', '55781', '17145', '48826', '6702', '12581', '17002', '26446', '48882', '17122', '51226', '17372', '79306', '73068', '97773', '57321', '54289', '44929', '85025', '95776', '61667', '13476', '41525', '44018', '73473', '21219', '52848', '95201', '22241', '46057', '23591', '51086', '32763', '80737', '46154', '16565', '96937', '98182', '81583', '23933', '3506', '77927', '18200', '8718', '16296', '80436', '71336', '88883', '52034', '81350', '63039', '83976', '10045', '6485', '23468', '58576', '67744', '21575', '19233', '25602', '40798', '95839', '73059', '21139', '1924', '96515', '68780', '90846', '4053', '31993', '42995', '1075', '73299', '18738', '59841', '99111', '23503', '63456', '53252', '97488', '50015', '83555', '66200', '99783', '21968', '59375', '22962', '51951', '93360', '28061', '17948', '17920', '69339', '86078', '65604', '89481', '74584', '66770', '50487', '3932', '10694', '83700', '13274', '46837', '60987', '99715', '42033', '12212', '5289', '55332', '56583', '47255', '5738', '80744', '59085', '75670', '82290', '57100', '88660', '45344', '28625', '6535', '48391', '15524', '51871', '43948', '7695', '21438', '743', '82512', '82111', '24157', '20598', '32711', '15963', '7381', '1892', '88691', '16619', '97599', '68543', '10124', '6017', '69498', '56264', '93662', '74727', '14098', '27542', '43870', '83561', '91581', '28073', '60393', '99115', '12807', '17457', '82641', '19055', '44123', '76261', '747', '90165', '124', '6069', '6539', '93596', '70119', '63925', '63359', '82148', '11764', '44373', '14058', '47266', '22499', '74866', '9289', '78182', '84461', '8985', '87497', '81786', '18687', '14542', '81378', '59222', '64136', '30477', '5506', '52269', '6809', '60868', '3798', '23780', '42255', '28707', '44486', '5254', '55679', '79894', '91904', '12586', '91881', '25915', '66152', '45805', '95022', '95076', '75856', '1182', '50417', '18676', '2228', '5476', '3792', '90396', '20124', '7452', '65411', '92475', '62835', '50827', '20354', '95957', '27338', '72328', '11949', '92331', '50093', '78956', '8426', '16436', '26398', '65513', '69778', '46315', '83691', '43664', '82432', '2906', '76480', '89292', '21030', '29470', '58773', '96785', '19102', '88809', '54929', '18403', '593', '3794', '95951', '9526', '68623', '70764', '20846', '66825', '49971', '3302', '80375', '5171', '7860', '91004', '46092', '86921', '10552', '62538', '66710', '59736', '81810', '83873'}\n",
      "\n",
      "✅ SUBJECT_ID 存在於 Yuran Dead 但不存在於 My Dead共 7:\n",
      "{'16924', '51277', '49140', '8141', '60753', '773', '72048'}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 398 entries, 1 to 2159\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   SUBJECT_ID            398 non-null    int64 \n",
      " 1   HADM_ID               398 non-null    int64 \n",
      " 2   ADMITTIME             398 non-null    object\n",
      " 3   DISCHTIME             398 non-null    object\n",
      " 4   DEATHTIME             398 non-null    object\n",
      " 5   HOSPITAL_EXPIRE_FLAG  398 non-null    int64 \n",
      " 6   ICUSTAY_ID            398 non-null    int64 \n",
      " 7   INTIME                398 non-null    object\n",
      " 8   OUTTIME               398 non-null    object\n",
      " 9   ICD9_CODE             398 non-null    int64 \n",
      " 10  ECG_OVERLAP           398 non-null    bool  \n",
      " 11  ECG_IN_ICU            398 non-null    bool  \n",
      "dtypes: bool(2), int64(5), object(5)\n",
      "memory usage: 35.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "merge_subject_filterd = pd.read_csv(\"./logs/merge_subject_filtered.csv\")\n",
    "def split_groups(df:pd.DataFrame)->Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Based on the HOSPITAL_EXPIRE_FLAG to split the group surv/mort\n",
    "    \"\"\"\n",
    "    surv  = df[df['HOSPITAL_EXPIRE_FLAG']==0].copy()\n",
    "    mort  = df[df['HOSPITAL_EXPIRE_FLAG']==1].copy()\n",
    "    return surv, mort\n",
    "surv,mort = split_groups(merge_subject_filterd)\n",
    "print(\"========  Stage 1 Result ==============\")\n",
    "print(f\"Survior Cohort: Number of Data {len(surv)}; Total Subject ID: {surv['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Deceased Cohort: Number of Data {len(mort)}; Total Subject ID: {mort['SUBJECT_ID'].nunique()}\")\n",
    "alive = pd.read_csv(\"./experiment_data_from_yuran/alive_42731_withHRV.csv\")\n",
    "dead = pd.read_csv(\"./experiment_data_from_yuran/dead_42731_withHRV.csv\")\n",
    "surv_cross = cross_validation_missing_subject(surv,alive, \"My Surv\",\"Yuran Surv\")\n",
    "dead_cross = cross_validation_missing_subject(mort,dead,\"My Dead\",\"Yuran Dead\")\n",
    "print(mort.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99d16b",
   "metadata": {},
   "source": [
    "# Step 2 : Survivor / Deceased Cohort Exclusion\n",
    "## Deceased Cohort Exclusion Criteria\n",
    "- Post-mortem ECG present\n",
    "    - T1 < ADMISSION.deathtime\n",
    "        - Issue: if ADMISSION.deathtime missed, filling with ADMISSION.DISCHTIME // Revised: 直接過濾掉\n",
    "- Age > 125 year\n",
    "    - age計算 : PATIENTS.dob + ADMISSION.admittime(入院時間)\n",
    "- no Continuous Lead II ECG >= 10 hr before death\n",
    "    - Total Signal Length >= 36000 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b655a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 398\n",
      "Total SUBJECT ID: 398\n",
      "After Merge Total Data: 398\n",
      "After merge SUBJECT ID: 398\n",
      "After Saving Total Data: 398\n",
      "After Saving SUBJECT ID: 398\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 14 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   SUBJECT_ID                     398 non-null    int64         \n",
      " 1   HADM_ID                        398 non-null    int64         \n",
      " 2   ADMITTIME                      398 non-null    datetime64[ns]\n",
      " 3   DISCHTIME                      398 non-null    object        \n",
      " 4   DEATHTIME                      398 non-null    object        \n",
      " 5   HOSPITAL_EXPIRE_FLAG           398 non-null    int64         \n",
      " 6   ICUSTAY_ID                     398 non-null    int64         \n",
      " 7   INTIME                         398 non-null    object        \n",
      " 8   OUTTIME                        398 non-null    object        \n",
      " 9   ICD9_CODE                      398 non-null    int64         \n",
      " 10  ECG_OVERLAP                    398 non-null    bool          \n",
      " 11  ECG_IN_ICU                     398 non-null    bool          \n",
      " 12  DOB                            398 non-null    datetime64[ns]\n",
      " 13  AGE_AT_ADMISSION_YYMMDDHHMMSS  398 non-null    object        \n",
      "dtypes: bool(2), datetime64[ns](2), int64(5), object(5)\n",
      "memory usage: 38.2+ KB\n",
      "None\n",
      "Total Data: 398\n",
      "Total SUBJECT ID: 398\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def merge_age_by_admit_mort(mort: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    將 Stage 1 Deceased Cohort 引入 patient.dob(病患出生日期)，並計算入院時年紀\n",
    "    - Issue: admissions.deathtime(死亡日期)缺失則以ICU離開時間填補(Issue: ICU OUTTIME 比 ADMISSIONS.DEATHTIME 晚 WTF)\n",
    "        - 0728:Update: 應該要直接排除\n",
    "\n",
    "    Args:\n",
    "    - mort : pd.DataFrame, Stage 1 Deceased Cohort\n",
    "\n",
    "    Return:\n",
    "    - mort_with_Age_Death: pd.DataFrame, 新增欄位 [\"DOB\",\"AGE_AT_ADMISSION_YYMMDDHHMMSS\"]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pat = pd.read_csv(os.path.join(DATA_CSV, \"PATIENTS.csv\"),\n",
    "                          usecols=['SUBJECT_ID', 'DOB'])\n",
    "        # adm = pd.read_csv(os.path.join(DATA_CSV, \"ADMISSIONS.csv\"),\n",
    "        #                   usecols=['SUBJECT_ID','HADM_ID' ,'DEATHTIME'])\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Required CSV files not found in {DATA_CSV}\")\n",
    "    \n",
    "    # 2. 轉成 pandas 的 datetime64（先做 basic 清理，coerce 會把壞格式設為 NaT）\n",
    "    pat['DOB']        = pd.to_datetime(pat['DOB'], errors='coerce')\n",
    "    mort['ADMITTIME']  = pd.to_datetime(mort['ADMITTIME'], errors='coerce')\n",
    "    # adm['DEATHTIME']  = pd.to_datetime(adm['DEATHTIME'], errors='coerce')\n",
    "    \n",
    "    # 3. 合併資料（mort 為基準，用 left join）\n",
    "    mort_with_Age_Death = (\n",
    "        mort\n",
    "        .merge(pat, on=\"SUBJECT_ID\", how=\"left\")                # PATIENTS 仍僅用 SUBJECT_ID\n",
    "        # .merge(                                                  # ADMISSIONS 用雙欄位對齊\n",
    "        #     adm[['SUBJECT_ID', 'HADM_ID', 'DEATHTIME']],        # 先保留要用的欄位\n",
    "        #     on=[\"SUBJECT_ID\", \"HADM_ID\"],                       # 這裡傳 list\n",
    "        #     how=\"left\"\n",
    "        # )\n",
    "    )\n",
    "    print(f'After Merge Total Data: {len(mort_with_Age_Death)}')\n",
    "    print(f'After merge SUBJECT ID: {mort_with_Age_Death[\"SUBJECT_ID\"].nunique()}')\n",
    "    \n",
    "    # 4. 定義逐列計算 age 的函式\n",
    "    def calc_age_ymdhms(row):\n",
    "        dob   = row['DOB']\n",
    "        admit = row['ADMITTIME']\n",
    "        # 如果有任一是 NaT，就回傳 NaN\n",
    "        if pd.isna(dob) or pd.isna(admit):\n",
    "            return np.nan\n",
    "        # 用 python datetime 計算差分\n",
    "        rd = relativedelta(admit.to_pydatetime(), dob.to_pydatetime())\n",
    "        # 格式化：YYYYMMDDHHMMSS\n",
    "        return (\n",
    "            f\"{rd.years:04d}\"\n",
    "            f\"{rd.months:02d}\"\n",
    "            f\"{rd.days:02d}\"\n",
    "            f\"{rd.hours:02d}\"\n",
    "            f\"{rd.minutes:02d}\"\n",
    "            f\"{rd.seconds:02d}\"\n",
    "        )\n",
    "    \n",
    "    # 5. 套用到整張表\n",
    "    mort_with_Age_Death['AGE_AT_ADMISSION_YYMMDDHHMMSS'] = mort_with_Age_Death.apply(calc_age_ymdhms, axis=1)\n",
    "    \n",
    "    if mort_with_Age_Death['DEATHTIME'].isnull().any():\n",
    "        # print(\"DEATHTIME 欄位檢測到缺失，以離開ICU時間做填補\")\n",
    "        # mort_with_Age_Death['DEATHTIME'] = mort_with_Age_Death['DEATHTIME'].fillna(mort_with_Age_Death['OUTTIME'])\n",
    "        print(\"DEATHTIME 欄位檢測到缺失，移除缺失欄位\")\n",
    "        mort_with_Age_Death= mort_with_Age_Death[~mort_with_Age_Death['DEATHTIME'].isnull()]\n",
    "\n",
    "    mort_with_Age_Death.to_csv(os.path.join(LOGS,\"test.csv\"),index= False)\n",
    "    print(f'After Saving Total Data: {len(mort_with_Age_Death)}')\n",
    "    print(f'After Saving SUBJECT ID: {mort_with_Age_Death[\"SUBJECT_ID\"].nunique()}')\n",
    "    print(mort_with_Age_Death.info())\n",
    "    return mort_with_Age_Death\n",
    "\n",
    "print(f'Total Data: {len(mort)}')\n",
    "print(f'Total SUBJECT ID: {mort[\"SUBJECT_ID\"].nunique()}')\n",
    "mort_with_Age_Death = merge_age_by_admit_mort(mort)\n",
    "\n",
    "print(f'Total Data: {len(mort_with_Age_Death)}')\n",
    "print(f'Total SUBJECT ID: {mort_with_Age_Death[\"SUBJECT_ID\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86748b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Stage II Deceased Cohort Result =========== \n",
      "Total Data: 242\n",
      "Total SUBJECT ID: 242\n"
     ]
    }
   ],
   "source": [
    "def filter_age_l2Time_postECG(mort_with_Age_Death:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    合併 ecg_match.csv (需要包含[T0,T1,LEAD2_SEC]等欄位)，並依照指定條件做篩選\n",
    "    Exclusion Criteria\n",
    "    - Age > 125 year\n",
    "    - Post ECG : T1 > DEATHTIME\n",
    "    - LEAD2_SEC < 36000 sec\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ecg_match = pd.read_csv(os.path.join(LOGS, \"ecg_match.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Required CSV files not found in {LOGS}\")\n",
    "\n",
    "    mort_with_Age_Death_ecg = mort_with_Age_Death.merge(ecg_match,on = ['SUBJECT_ID'],how = \"left\")\n",
    "\n",
    "    for col in ['T0', 'T1']:\n",
    "        mort_with_Age_Death_ecg[col] = pd.to_datetime(mort_with_Age_Death_ecg[col], errors='coerce')\n",
    "    if mort_with_Age_Death_ecg['T0'].isna().any() or mort_with_Age_Death_ecg['T1'].isna().any():\n",
    "        n0 = mort_with_Age_Death_ecg['T0'].isna().sum()\n",
    "        n1 = mort_with_Age_Death_ecg['T1'].isna().sum()\n",
    "        print(f\"Warning: T0 缺漏 {n0} 筆, T1 缺漏 {n1} 筆 → 已刪除這些筆資料\")\n",
    "        mort_with_Age_Death_ecg = mort_with_Age_Death_ecg.dropna(subset=['T0','T1']).reset_index(drop=True)\n",
    "    \n",
    "    # 構建篩選條件\n",
    "    # 1. 年齡 <= 125\n",
    "    mort_with_Age_Death_ecg['AGE_YEARS'] = (\n",
    "    mort_with_Age_Death_ecg['AGE_AT_ADMISSION_YYMMDDHHMMSS']\n",
    "        .astype(str).str.zfill(14).str[:4]        # 取前 4 位：0000‒9999 年\n",
    "        .astype(int)                              # 轉成整數\n",
    "    )\n",
    "    mask_age = mort_with_Age_Death_ecg['AGE_YEARS'] <= 125\n",
    "\n",
    "    # 2 ECG 在死亡前(T1 <= DEATHTIME)\n",
    "    mort_with_Age_Death_ecg['DEATHTIME'] = pd.to_datetime(mort_with_Age_Death_ecg['DEATHTIME'], errors='coerce')\n",
    "    mask_post = (mort_with_Age_Death_ecg['T1'] <= mort_with_Age_Death_ecg['DEATHTIME']) \n",
    "\n",
    "    # 3 LEAD2_SEC >= 36000\n",
    "    mask_lead = mort_with_Age_Death_ecg['LEAD2_SEC'] >= 36000\n",
    "\n",
    "    # 保留所有判斷結果方便複查\n",
    "    mort_with_Age_Death_ecg['AGE_UNDER_125'] = mask_age\n",
    "    mort_with_Age_Death_ecg['WIOUT_POST_ECG'] = mask_post\n",
    "    mort_with_Age_Death_ecg['ENOUGH_LEAD2'] = mask_lead\n",
    "    # 最終篩選\n",
    "    mort_stage2_filtered = mort_with_Age_Death_ecg.loc[mask_age & mask_post & mask_lead].reset_index(drop=True)\n",
    "\n",
    "    mort_stage2_filtered.to_csv(os.path.join(LOGS,\"mort_stage2_filtered.csv\"),index = False)\n",
    "    mort_with_Age_Death_ecg.to_csv(os.path.join(LOGS,\"mort_stage2_detailed.csv\"),index = False)\n",
    "\n",
    "    return mort_stage2_filtered\n",
    "\n",
    "mort_stage2_filtered = filter_age_l2Time_postECG(mort_with_Age_Death)\n",
    "\n",
    "print(f\"=========== Stage II Deceased Cohort Result =========== \")\n",
    "print(f'Total Data: {len(mort_stage2_filtered)}')\n",
    "print(f'Total SUBJECT ID: {mort_stage2_filtered[\"SUBJECT_ID\"].nunique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe654b97",
   "metadata": {},
   "source": [
    "## Survivor Cohort Exclusion Criteria\n",
    "\n",
    "- Age > 125 year\n",
    "    - age計算 : PATIENTS.dob + ADMISSION.admittime(入院時間)\n",
    "- no Continuous Lead II ECG >= 10 hr before discharge\n",
    "    - Total Signal Length >= 36000 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c66f60f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 1775\n",
      "total ID: 1775\n",
      "SUBJECT_ID\n",
      "98769    1\n",
      "85       1\n",
      "214      1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n經分析剛好兩筆時間資料接近資料重疊，屬於篩選條件的edge case，只有最後一筆為正確資料\\n處理方式: 主動刪除 subject id : 93432, hadm id : 190234 , icustay_id: 204538 的該筆資料\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exception Analsyis : Duplicated SUBJECT ID 93432\n",
    "print(f\"Total data: {len(surv['SUBJECT_ID'])}\")\n",
    "print(f\"total ID: {surv['SUBJECT_ID'].nunique()}\")\n",
    "print(surv['SUBJECT_ID'].value_counts()[0:3])\n",
    "\"\"\"\n",
    "經分析剛好兩筆時間資料接近資料重疊，屬於篩選條件的edge case，只有最後一筆為正確資料\n",
    "處理方式: 主動刪除 subject id : 93432, hadm id : 190234 , icustay_id: 204538 的該筆資料\n",
    "\"\"\"\n",
    "\n",
    "# surv_remove_edgeCase = surv.drop(surv[surv[\"ICUSTAY_ID\"] == 204538].index).reset_index(drop=True)\n",
    "# print(f\"After Remove Total data: {len(surv_remove_edgeCase['SUBJECT_ID'])}\")\n",
    "# print(f\"After Remove total ID: {surv_remove_edgeCase['SUBJECT_ID'].nunique()}\")\n",
    "# print(surv_remove_edgeCase.info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d708203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Merge Total Data: 1775\n",
      "After merge SUBJECT ID: 1775\n",
      "After Saving Total Data: 1775\n",
      "After Saving SUBJECT ID: 1775\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1775 entries, 0 to 1774\n",
      "Data columns (total 14 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   SUBJECT_ID                     1775 non-null   int64         \n",
      " 1   HADM_ID                        1775 non-null   int64         \n",
      " 2   ADMITTIME                      1775 non-null   datetime64[ns]\n",
      " 3   DISCHTIME                      1775 non-null   object        \n",
      " 4   DEATHTIME                      0 non-null      object        \n",
      " 5   HOSPITAL_EXPIRE_FLAG           1775 non-null   int64         \n",
      " 6   ICUSTAY_ID                     1775 non-null   int64         \n",
      " 7   INTIME                         1775 non-null   object        \n",
      " 8   OUTTIME                        1775 non-null   object        \n",
      " 9   ICD9_CODE                      1775 non-null   int64         \n",
      " 10  ECG_OVERLAP                    1775 non-null   bool          \n",
      " 11  ECG_IN_ICU                     1775 non-null   bool          \n",
      " 12  DOB                            1775 non-null   datetime64[ns]\n",
      " 13  AGE_AT_ADMISSION_YYMMDDHHMMSS  1775 non-null   object        \n",
      "dtypes: bool(2), datetime64[ns](2), int64(5), object(5)\n",
      "memory usage: 170.0+ KB\n",
      "最終缺失值檢測: None\n"
     ]
    }
   ],
   "source": [
    "def merge_age_by_admit_surv(surv_remove_edgeCase: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    將 Stage 1 surv Cohort 引入 patient.dob(病患出生日期)，並計算入院時年紀\n",
    "    Args:\n",
    "    - surv_remove_edgeCase : pd.DataFrame, Stage 1 Surv Cohort 移除所有重複 subject id 結果\n",
    "\n",
    "    Return:\n",
    "    - surv_with_Age: pd.DataFrame, 新增欄位 [\"DOB\",\"AGE_AT_ADMISSION_YYMMDDHHMMSS\"]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pat = pd.read_csv(os.path.join(DATA_CSV, \"PATIENTS.csv\"),\n",
    "                          usecols=['SUBJECT_ID', 'DOB'])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Required CSV files not found in {DATA_CSV}\")\n",
    "    \n",
    "    # 2. 轉成 pandas 的 datetime64（先做 basic 清理，coerce 會把壞格式設為 NaT）\n",
    "    pat['DOB']        = pd.to_datetime(pat['DOB'], errors='coerce')\n",
    "    surv_remove_edgeCase['ADMITTIME']  = pd.to_datetime(surv_remove_edgeCase['ADMITTIME'], errors='coerce')\n",
    "    # mask_bad = surv_remove_edgeCase['ADMITTIME'].isna()\n",
    "    # print(\"被轉 NaT 的行數:\", mask_bad.sum())\n",
    "    # print(surv_remove_edgeCase.loc[mask_bad, ['SUBJECT_ID', 'ADMITTIME']].head())\n",
    "\n",
    "    \n",
    "    # 3. 合併資料（surv_remove_edgeCase為基準，用 left join）\n",
    "    surv_with_Age = (\n",
    "        surv_remove_edgeCase\n",
    "        .merge(pat, on=\"SUBJECT_ID\", how=\"left\")                # PATIENTS 仍僅用 SUBJECT_ID\n",
    "    )\n",
    "    print(f'After Merge Total Data: {len(surv_with_Age)}')\n",
    "    print(f'After merge SUBJECT ID: {surv_with_Age[\"SUBJECT_ID\"].nunique()}')\n",
    "    \n",
    "    # 4. 定義逐列計算 age 的函式\n",
    "    def calc_age_ymdhms(row):\n",
    "        dob   = row['DOB']\n",
    "        admit = row['ADMITTIME']\n",
    "        # 如果有任一是 NaT，就回傳 NaN\n",
    "        if pd.isna(dob) or pd.isna(admit):\n",
    "            return np.nan\n",
    "        # 用 python datetime 計算差分\n",
    "        rd = relativedelta(admit.to_pydatetime(), dob.to_pydatetime())\n",
    "        # 格式化：YYYYMMDDHHMMSS\n",
    "        return (\n",
    "            f\"{rd.years:04d}\"\n",
    "            f\"{rd.months:02d}\"\n",
    "            f\"{rd.days:02d}\"\n",
    "            f\"{rd.hours:02d}\"\n",
    "            f\"{rd.minutes:02d}\"\n",
    "            f\"{rd.seconds:02d}\"\n",
    "        )\n",
    "    \n",
    "    # 5. 套用到整張表\n",
    "    surv_with_Age['AGE_AT_ADMISSION_YYMMDDHHMMSS'] = surv_with_Age.apply(calc_age_ymdhms, axis=1)\n",
    "\n",
    "    surv_with_Age.to_csv(os.path.join(LOGS,\"test.csv\"),index= False)\n",
    "    print(f'After Saving Total Data: {len(surv_with_Age)}')\n",
    "    print(f'After Saving SUBJECT ID: {surv_with_Age[\"SUBJECT_ID\"].nunique()}')\n",
    "    print(f'最終缺失值檢測: {surv_with_Age.info()}')\n",
    "    return surv_with_Age\n",
    "\n",
    "surv_with_Age = merge_age_by_admit_surv(surv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdb6c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Stage II S Cohort Result =========== \n",
      "Total Data: 1448\n",
      "Total SUBJECT ID: 1448\n"
     ]
    }
   ],
   "source": [
    "def filter_age_l2Time(surv_with_Age:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    合併 ecg_match.csv (需要包含[T0,T1,LEAD2_SEC]等欄位)，並依照指定條件做篩選\n",
    "    Exclusion Criteria\n",
    "    - Age > 125 year\n",
    "    - LEAD2_SEC < 36000 sec\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ecg_match = pd.read_csv(os.path.join(LOGS, \"ecg_match.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Required CSV files not found in {LOGS}\")\n",
    "\n",
    "    surv_with_Age_ecg = surv_with_Age.merge(ecg_match,on = ['SUBJECT_ID'],how = \"left\")\n",
    "\n",
    "    for col in ['T0', 'T1']:\n",
    "        surv_with_Age_ecg[col] = pd.to_datetime(surv_with_Age_ecg[col], errors='coerce')\n",
    "    if surv_with_Age_ecg['T0'].isna().any() or surv_with_Age_ecg['T1'].isna().any():\n",
    "        n0 = surv_with_Age_ecg['T0'].isna().sum()\n",
    "        n1 = surv_with_Age_ecg['T1'].isna().sum()\n",
    "        print(f\"Warning: T0 缺漏 {n0} 筆, T1 缺漏 {n1} 筆 → 已刪除這些筆資料\")\n",
    "        surv_with_Age_ecg = surv_with_Age_ecg.dropna(subset=['T0','T1']).reset_index(drop=True)\n",
    "    \n",
    "    # 構建篩選條件\n",
    "    # 1. 年齡 <= 125\n",
    "    surv_with_Age_ecg['AGE_YEARS'] = (\n",
    "    surv_with_Age_ecg['AGE_AT_ADMISSION_YYMMDDHHMMSS']\n",
    "        .astype(str).str.zfill(14).str[:4]        # 取前 4 位：0000‒9999 年\n",
    "        .astype(int)                              # 轉成整數\n",
    "    )\n",
    "    mask_age = surv_with_Age_ecg['AGE_YEARS'] <= 125\n",
    "\n",
    "\n",
    "    # 3 LEAD2_SEC >= 36000\n",
    "    mask_lead = surv_with_Age_ecg['LEAD2_SEC'] >= 36000\n",
    "\n",
    "    # 保留所有判斷結果方便複查\n",
    "    surv_with_Age_ecg['AGE_UNDER_125'] = mask_age\n",
    "    surv_with_Age_ecg['ENOUGH_LEAD2'] = mask_lead\n",
    "    # 最終篩選\n",
    "    surv_stage2_filtered = surv_with_Age_ecg.loc[mask_age & mask_lead].reset_index(drop=True)\n",
    "\n",
    "    surv_stage2_filtered.to_csv(os.path.join(LOGS,\"surv_stage2_filtered.csv\"),index = False)\n",
    "    surv_with_Age_ecg.to_csv(os.path.join(LOGS,\"surv_stage2_detailed.csv\"),index = False)\n",
    "\n",
    "    return surv_stage2_filtered\n",
    "\n",
    "surv_stage2_filtered = filter_age_l2Time(surv_with_Age)\n",
    "\n",
    "print(f\"=========== Stage II S Cohort Result =========== \")\n",
    "print(f'Total Data: {len(surv_stage2_filtered)}')\n",
    "print(f'Total SUBJECT ID: {surv_stage2_filtered[\"SUBJECT_ID\"].nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efd81b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{17667, 94853, 84874, 52746, 42510, 44437, 54935, 19866, 82843, 29730, 45608, 78892, 91181, 85551, 5685, 47677, 86209, 94785, 83782, 26055, 92235, 12365, 82000, 62035, 77524, 88921, 30170, 5727, 60641, 67429, 80106, 53102, 41976, 40569, 63486}\n",
      "35\n",
      "1117\n",
      "以下 SUBJECT_ID 未通過時間規則檢查：\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['MIN_OUT'] = outlier[['OUTTIME', 'DISCHTIME']].min(axis=1)\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['RULE1_PASSED'] = outlier['T1'] < outlier['MIN_OUT']\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['MAX_IN'] = outlier[['INTIME', 'ADMITTIME']].max(axis=1)\n",
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_40124\\3127461657.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['RULE2_PASSED'] = outlier['T0'] > outlier['MAX_IN']\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "alive = pd.read_csv(\"./experiment_data_from_yuran/alive_42731_withHRV.csv\")\n",
    "alive_set = set(alive['SUBJECT_ID'].to_list())\n",
    "surv_stage2_set = set(surv_stage2_filtered['SUBJECT_ID'])\n",
    "print(alive_set-surv_stage2_set)\n",
    "print(len(alive_set-surv_stage2_set))\n",
    "print(len(surv_stage2_set-alive_set))\n",
    "\n",
    "# Check Surv Rule\n",
    "# T1<ICU_OUT / DISCHTIME (取時間更早的)\n",
    "outlier = surv_stage2_filtered[surv_stage2_filtered['SUBJECT_ID'].isin(alive_set & surv_stage2_set)]\n",
    "\n",
    "# 檢查時間欄位：轉為 datetime（保險起見，避免格式問題）\n",
    "for col in ['T0', 'T1', 'OUTTIME', 'DISCHTIME', 'INTIME', 'ADMITTIME']:\n",
    "    outlier[col] = pd.to_datetime(outlier[col], errors='coerce')\n",
    "\n",
    "# --- 規則 1：T1 < min(OUTTIME, DISCHTIME) ---\n",
    "outlier['MIN_OUT'] = outlier[['OUTTIME', 'DISCHTIME']].min(axis=1)\n",
    "outlier['RULE1_PASSED'] = outlier['T1'] < outlier['MIN_OUT']\n",
    "\n",
    "# --- 規則 2：T0 > max(INTIME, ADMITTIME) ---\n",
    "outlier['MAX_IN'] = outlier[['INTIME', 'ADMITTIME']].max(axis=1)\n",
    "outlier['RULE2_PASSED'] = outlier['T0'] > outlier['MAX_IN']\n",
    "\n",
    "# 篩出任一規則不通過的 row\n",
    "violated = outlier[(~outlier['RULE1_PASSED']) | (~outlier['RULE2_PASSED'])]\n",
    "\n",
    "# 取得違反規則的 subject_id（避免重複）\n",
    "violated_subject_ids = violated['SUBJECT_ID'].unique()\n",
    "\n",
    "print(\"以下 SUBJECT_ID 未通過時間規則檢查：\")\n",
    "print(len(violated_subject_ids))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a12ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alive = pd.read_csv(\"./experiment_data_from_yuran/alive_42731_withHRV.csv\")\n",
    "alive_set = set(alive['SUBJECT_ID'].to_list())\n",
    "surv_stage2 = pd.read_csv(\".\\logs\\surv_stage2_detailed.csv\")\n",
    "\n",
    "lead2Time = surv_stage2[surv_stage2['SUBJECT_ID'].isin(alive_set)]\n",
    "lead2Time_fail = lead2Time.loc[lead2Time['AGE_UNDER_125'] & ~lead2Time['ENOUGH_LEAD2']]\n",
    "print(lead2Time_fail['SUBJECT_ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d69d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 0 :\n",
      "Detail: set()\n",
      "total : 0 :\n",
      "Detail: set()\n"
     ]
    }
   ],
   "source": [
    "base = pd.read_csv(r\".\\logs\\base.csv\")\n",
    "dead = pd.read_csv(r\"./experiment_data_from_yuran/dead_42731_withHRV.csv\")\n",
    "dead_set = set(dead['SUBJECT_ID'].to_list())\n",
    "\n",
    "base_set = set(base['SUBJECT_ID'].to_list())\n",
    "print(f\"total : {len(alive_set-base_set)} :\\nDetail: {alive_set-base_set}\")\n",
    "print(f\"total : {len(dead_set-base_set)} :\\nDetail: {dead_set-base_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a42b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "def parse_base_datetime(hdr):\n",
    "    \"\"\"\n",
    "    處理 Header base time data type (適用於不同 wfdb 版本)\n",
    "    Args:\n",
    "    - hdr: wfdb.header 物件\n",
    "\n",
    "    Return:\n",
    "    - t0: 紀錄的起始時間, datetime 物件\n",
    "    \"\"\"\n",
    "    if getattr(hdr, \"base_datetime\", None):           # v4.x 直接提供\n",
    "        return hdr.base_datetime\n",
    "    # base_date 可能已是 datetime.date\n",
    "    bdate = hdr.base_date if isinstance(hdr.base_date, date) \\\n",
    "            else datetime.strptime(hdr.base_date, \"%d/%m/%Y\").date()\n",
    "    # base_time 可能已是 datetime.time\n",
    "    btime = hdr.base_time if isinstance(hdr.base_time, time) \\\n",
    "            else datetime.strptime(hdr.base_time.split(\".\")[0], \"%H:%M:%S\").time()\n",
    "    return datetime.combine(bdate, btime)\n",
    "\n",
    "def last_lead2_segment_end(hdr, rec_dir: str) -> Optional[Tuple[datetime, float]]:\n",
    "    \"\"\"\n",
    "    計算每一個 含有 Lead II segment 的 ECG 時間長度(in sec)，並以最後一個含 Lead II 的 segment 的結束作為整段ECG訊號的結束(即 T1)\n",
    "\n",
    "    Args:\n",
    "    - hdr: wfdb.rdheader() 回傳的 MultiRecordHeader 物件\n",
    "    - rec_dir: 該 header 檔案所在資料夾 (Path，不含 .hea)\n",
    "\n",
    "    Return\n",
    "    - (T1, total_lead2_sec): \n",
    "        T1 為最後一個 Lead II segment 的結束 datetime\n",
    "        total_lead2_sec 為所有 segment 中包含 Lead II 的總秒數\n",
    "      若完全沒有 Lead II，回傳 None。\n",
    "    \"\"\"\n",
    "    base_dt = parse_base_datetime(hdr)\n",
    "    if base_dt is None:\n",
    "        return None\n",
    "\n",
    "    # 1) 取段名與長度（nsamp） —— 來源 hdr.seg_name / hdr.seg_len\n",
    "    seg_names = hdr.seg_name           # list[str]\n",
    "    seg_lens  = hdr.seg_len            # list[int]  :contentReference[oaicite:3]{index=3}\n",
    "    assert len(seg_names) == len(seg_lens)\n",
    "\n",
    "    # 2) 預先算 cumulative start index\n",
    "    cum = [0]\n",
    "    for L in seg_lens[:-1]:\n",
    "        cum.append(cum[-1] + L)\n",
    "\n",
    "    total_sec = 0.0\n",
    "    last_end  = None\n",
    "\n",
    "    # 3) 倒序掃描段\n",
    "    for seg_name, start_idx, seg_len in reversed(list(zip(seg_names, cum, seg_lens))):\n",
    "        if seg_name == \"~\":            # gap 段 :contentReference[oaicite:4]{index=4}\n",
    "            continue\n",
    "        sub_hdr = wfdb.rdheader(Path(rec_dir, seg_name))\n",
    "        if \"II\" not in [n.upper().replace(\" \", \"\") for n in sub_hdr.sig_name]:\n",
    "            continue\n",
    "\n",
    "        print(f\"file : {seg_name} : sigal length : {seg_len} , fs: {sub_hdr.fs}\")\n",
    "\n",
    "        seg_sec = seg_len / sub_hdr.fs\n",
    "        total_sec += seg_sec\n",
    "        \n",
    "        if last_end is None: #找到最後一段，先行紀錄\n",
    "            seg_start_abs = base_dt + timedelta(seconds=start_idx / hdr.fs)\n",
    "            last_end = seg_start_abs + timedelta(seconds=seg_sec)\n",
    "\n",
    "\n",
    "    if last_end is None:\n",
    "        return None\n",
    "    return last_end, total_sec\n",
    "\n",
    "def header_time_range(header_path:str)->Optional[Tuple[datetime,datetime,float]]:\n",
    "    \"\"\"\n",
    "    計算 ECG訊號的起始時間，關於時間算法的界定如下:\n",
    "    - T0: ECG 訊號起始時間，這裡採用 `header.base_dt`: 整個 multi-segment record 的起始時間，而不是某個 segment 的起始時間\n",
    "        - 更精確做法應該採用 seg 內段落(待考慮，予以保留)\n",
    "    - T1: ECG 訊號結束時間\n",
    "        - 採用最後一筆 LEAD II seg 作為ECG訊號截止時間\n",
    "        - 計算方式: \n",
    "            - seg_end = seg_start + (seg_length_in_seconds)\n",
    "            - seg_length_in_seconds = sub_hdr.sig_len / sub_hdr.fs\n",
    "    Arg:\n",
    "    - header_path: WFDB header 的完整path : Z:/p00/p000085/p000085-2167-07-25-21-11.hea\n",
    "    \"\"\"\n",
    "    rec_path   = Path(header_path).with_suffix(\"\")    # 去掉 .hea\n",
    "    hdr        = wfdb.rdheader(str(rec_path), rd_segments=True)\n",
    "    if hdr.base_time is None or hdr.base_date is None:\n",
    "        print(f\"{rec_path.name}: missing base_date/time\")\n",
    "        return None\n",
    "\n",
    "    t0 = parse_base_datetime(hdr)\n",
    "    res = last_lead2_segment_end(hdr, rec_path.parent)\n",
    "    if res is None:\n",
    "        print(f\"{rec_path.name}: 不存在 Lead II\")\n",
    "        return None\n",
    "\n",
    "    t1, l2_sec = res\n",
    "    return t0, t1, l2_sec\n",
    "\n",
    "print(header_time_range(\"z:\\p09\\p095396\\p095396-2142-11-06-12-12.hea\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
