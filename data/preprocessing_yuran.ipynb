{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12525e88",
   "metadata": {},
   "source": [
    "# FIle Structure and Vraiable Definition\n",
    "- `origin_data`: From DB 的原始資料\n",
    "- `origin_data_csv`: 原始資料轉檔為 `.csv`\n",
    "- `logs`: 實驗輸出檔案資料夾\n",
    "- `Z:` : 網路磁碟機(WFDB NAS)\n",
    "- `alive_yuran` : alive_42731_withHRV.csv, pd.Dataframe\n",
    "- `dead_yuran` : dead_42731_withHRV.csv, pd.Dataframe\n",
    "- `alive_set` : alive_yuran 的 subject_id 集合, set\n",
    "- `dead_set` : dead_yuran 的 subject_id 集合, set\n",
    "- `total_set` : `alive_set` and `dead_set` 的聯集, set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8487073",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN_DATA = \"origin_data\"\n",
    "DATA_CSV = \"origin_data_csv\"\n",
    "LOGS = \"logs\"\n",
    "MATCH = \"Z:\"\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import wfdb                                      # 讀取 WFDB header / record :contentReference[oaicite:4]{index=4}\n",
    "from pathlib import Path                         # 物件導向檔案操作 :contentReference[oaicite:5]{index=5}\n",
    "from datetime import datetime, timedelta, date, time\n",
    "from tqdm import tqdm                            # 進度列（可省略）\n",
    "import logging, os                               # 紀錄檔與系統路徑\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Optional, Tuple,Set\n",
    "from  tqdm import tqdm\n",
    "\n",
    "def cross_validation_missing_subject(fileA: str | pd.DataFrame, fileB: str | pd.DataFrame, fileAname:str, fileBname:str):\n",
    "    \"\"\"\n",
    "    列出\n",
    "    - 哪些 subject id 存在於 fileA 但不存在於 fileB\n",
    "    - 哪些 subject id 存在於 fileB 但不存在於 fileA\n",
    "    並將結果列出\n",
    "\n",
    "    Arg:\n",
    "    - fileA: file path of fileA\n",
    "    - fileB: file path of fileB\n",
    "    \"\"\"\n",
    "    if not isinstance(fileA,pd.DataFrame):\n",
    "        fileA_df = pd.read_csv(fileA)\n",
    "    else:\n",
    "        fileA_df = fileA\n",
    "\n",
    "    if not isinstance(fileB,pd.DataFrame):\n",
    "        fileB_df = pd.read_csv(fileB)\n",
    "    else:\n",
    "        fileB_df = fileB\n",
    "        \n",
    "\n",
    "    # 確認欄位名稱（假設欄位叫 SUBJECT_ID）\n",
    "    if 'SUBJECT_ID' not in fileA_df.columns or 'SUBJECT_ID' not in fileB_df.columns:\n",
    "        raise ValueError(\"Both files must contain 'SUBJECT_ID' column\")\n",
    "\n",
    "    # 轉換為集合\n",
    "    setA = set(fileA_df['SUBJECT_ID'].dropna().astype(str))\n",
    "    setB = set(fileB_df['SUBJECT_ID'].dropna().astype(str))\n",
    "\n",
    "    # 找差集\n",
    "    only_in_A = setA - setB\n",
    "    only_in_B = setB - setA\n",
    "\n",
    "    print(f\"✅ SUBJECT_ID 存在於 {fileAname} 但不存在於 {fileBname}, 共 {len(only_in_A)}:\")\n",
    "    print(only_in_A if only_in_A else \"無\")\n",
    "    \n",
    "    print(f\"\\n✅ SUBJECT_ID 存在於 {fileBname} 但不存在於 {fileAname}共 {len(only_in_B)}:\")\n",
    "    print(only_in_B if only_in_B else \"無\")\n",
    "\n",
    "    # 回傳結果（以 dict）\n",
    "    return {\n",
    "        \"only_in_A\": only_in_A,\n",
    "        \"only_in_B\": only_in_B\n",
    "    }\n",
    "\n",
    "try:\n",
    "    alive_yuran = pd.read_csv(\"./experiment_data_from_yuran/alive_42731_withHRV.csv\")\n",
    "    dead_yuran = pd.read_csv(\"./experiment_data_from_yuran/dead_42731_withHRV.csv\")\n",
    "\n",
    "    alive_set = set(alive_yuran['SUBJECT_ID'].to_list())\n",
    "    dead_set = set(dead_yuran[\"SUBJECT_ID\"].to_list())\n",
    "\n",
    "    total_set = alive_set | dead_set\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d18988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs folder exist.\n",
      "Logger module create success.\n"
     ]
    }
   ],
   "source": [
    "# ================================= 初始化紀錄 =================================\n",
    "\n",
    "if os.path.isdir(LOGS):\n",
    "    print(f\"{LOGS} folder exist.\")\n",
    "else:\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    print(f\"{LOGS} folder doesn't exist, creating new {LOGS}\")\n",
    "\n",
    "# 建立 Logger\n",
    "try:\n",
    "    logger = logging.getLogger(\"data_clean\")\n",
    "    logger.setLevel(logging.WARNING)  # WARNING 以上都會被記錄\n",
    "\n",
    "    # 建立 FileHandler，寫入 logs/clean.log\n",
    "    fh = logging.FileHandler(f\"{LOGS}/clean.log\", mode=\"w\", encoding=\"utf-8\")\n",
    "    # 只輸出訊息本身：SUBJECT_ID REASON\n",
    "    formatter = logging.Formatter(\"%(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 避免重複加入 handler\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(fh)\n",
    "    print(\"Logger module create success.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(e)\n",
    "\n",
    "def record_log(subject_id: str, reason: str):\n",
    "    \"\"\"\n",
    "    將不合格的資料記錄到 logs/clean.log。\n",
    "    例如： logger.warning(\"12345 invalid_date\")\n",
    "    \"\"\"\n",
    "    logger.warning(f\"{subject_id} {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23c7e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "只取每個病人最後一筆 ICU紀錄\n",
      "總資料:  10252\n",
      "總共多少 subject id 10252\n",
      "Missing ID (total 0): set()\n"
     ]
    }
   ],
   "source": [
    "# ================================= 篩選基本 Clinical Data: Output: base.csv =================================\n",
    "def load_clinical_tables()->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    篩選 具備 ICD Code 42731診斷碼與ICU紀錄的病患，***並且只保留該病患最後一筆ICUSTAY紀錄***\n",
    "    Return:\n",
    "    - base : pd.DataFrame, 患者綜合表格，包含以下資訊\n",
    "        來自 patients.csv\n",
    "        - SUBJECT_ID：患者唯一識別碼。\n",
    "\n",
    "        admissions 表格\n",
    "        - HADM_ID：住院請求編號，用於唯一識別一次住院。\n",
    "        - ADMITTIME：病患入院時間（時間戳記）。\n",
    "        - DISCHTIME：病患出院時間（時間戳記）。\n",
    "        - HOSPITAL_EXPIRE_FLAG：出院時是否死亡（1 = 出院時已逝，0 = 否）。\n",
    "        - DEATHTIME: 院內死亡時間\n",
    "\n",
    "        icustays 表格\n",
    "        - ICUSTAY_ID：ICU 住院期間的唯一識別符。\n",
    "        - INTIME：進入 ICU 的時間戳記。\n",
    "        - OUTTIME：離開 ICU 的時間戳記。\n",
    "\n",
    "        diagnoses_icd 表格\n",
    "        - ICD9_CODE：使用 ICD‑9 編碼系統記錄的診斷代碼（最多 6 位字元，包含空格，有些是 V 開頭代碼）\n",
    "    \"\"\"\n",
    "    adm   = pd.read_csv(os.path.join(DATA_CSV,\"ADMISSIONS.csv\"), usecols=['SUBJECT_ID','HADM_ID',\n",
    "                                                        'ADMITTIME','DISCHTIME','DEATHTIME',\n",
    "                                                        'HOSPITAL_EXPIRE_FLAG'])\n",
    "\n",
    "    icu   = pd.read_csv(os.path.join(DATA_CSV,\"ICUSTAYS.csv\"),\n",
    "                        usecols=['SUBJECT_ID','ICUSTAY_ID','HADM_ID','INTIME','OUTTIME'])\n",
    "\n",
    "    diag  = pd.read_csv(os.path.join(DATA_CSV,\"DIAGNOSES_ICD.csv\"),\n",
    "                        usecols=['SUBJECT_ID','HADM_ID','ICD9_CODE'])\n",
    "    # ICD9 42731 = AF\n",
    "    # ── 1. 找出 AF 病人 ───────────────────────\n",
    "    af_subjects = diag.loc[diag['ICD9_CODE']=='42731', 'SUBJECT_ID'].unique()\n",
    "\n",
    "    # ── 2. 保留這些病人的全部 ICU stay ───────\n",
    "    icu_af = icu[icu['SUBJECT_ID'].isin(af_subjects)]\n",
    "\n",
    "    # ── 3. 加入 ADMISSIONS 資料 ───────────────\n",
    "    base = icu_af.merge(adm, on=['SUBJECT_ID', 'HADM_ID'], how='left')\n",
    "\n",
    "    # ── 4. 標記該次住院是否含 AF ──────\n",
    "    af_flag = (diag[diag['ICD9_CODE']=='42731']\n",
    "            [['SUBJECT_ID','HADM_ID']]\n",
    "            .drop_duplicates()\n",
    "            .assign(HAS_AF=1))\n",
    "    base = base.merge(af_flag,\n",
    "                    on=['SUBJECT_ID','HADM_ID'],\n",
    "                    how='left') \\\n",
    "            .fillna({'HAS_AF': 0})\n",
    "\n",
    "    # ── 5. 去重（若 diag 同 HADM_ID 多筆 42731）──\n",
    "    base = base.drop_duplicates(subset=['ICUSTAY_ID'])\n",
    "\n",
    "    # 刪除非 AF 患者\n",
    "    base = base[base['HAS_AF'] == 1]\n",
    "\n",
    "    # (New) 只保留最後一筆 ICU 紀錄\n",
    "    print(\"只取每個病人最後一筆 ICU紀錄\")\n",
    "    # 先依 SUBJECT_ID 升序、INTIME 降序排序\n",
    "    base = base.sort_values(by=['SUBJECT_ID', 'INTIME'], ascending=[True, False])\n",
    "    # 對每個病人取第一筆（也就是最後一筆 ICU）\n",
    "    base = base.drop_duplicates(subset='SUBJECT_ID', keep='first')\n",
    "\n",
    "    # 儲存\n",
    "    base.to_csv(os.path.join(LOGS, \"base.csv\"), index=False)\n",
    "    return base\n",
    "\n",
    "base = load_clinical_tables()\n",
    "\n",
    "print(\"總資料: \",len(base))\n",
    "print(\"總共多少 subject id\",base['SUBJECT_ID'].nunique())\n",
    "\n",
    "\"\"\"Analysis with Final Set\"\"\"\n",
    "print(f\"Missing ID (total {len(total_set - set(base['SUBJECT_ID'].to_list()))}): {total_set - set(base['SUBJECT_ID'].to_list())}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae96515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records 2237 matched!\n"
     ]
    }
   ],
   "source": [
    "# ================================= 根據 base.csv 結果篩選 WFDB ECG 訊號: 該 Module 移送至 ecg_cache.py->Output: ecg_match_info.csv  =================================\n",
    "# base.csv: 保證 subject id 不重複\n",
    "# ecg_match_info.csv: 單一subject id 會對應到多筆 header\n",
    "#此階段 Output: ecg_match_info.csv\n",
    "\n",
    "\n",
    "def match_base_ecg(base:pd.DataFrame, ecg_match_info:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    針對每一筆 base（住院資料）依據 SUBJECT_ID，在 ecg_match_info（ECG 檔案對應表）中搜尋是否有時間區間重疊的 ECG 訊號資料。\n",
    "    若同時有多筆重疊，僅擷取 T1_lead2 最晚結束時間那筆；若只有一筆則直接擷取；若無符合則填 None。\n",
    "    最終回傳一份含有原 base 欄位與所有對應 ECG 資訊的新 DataFrame，並將結果存檔為 base_match_ecg_df.csv。\n",
    "\n",
    "    Args:\n",
    "    - base: pd.DataFrame  \n",
    "        住院資料，需包含 'SUBJECT_ID', 'INTIME', 'OUTTIME' 等欄位，作為比對主體。\n",
    "    - ecg_match_info: pd.DataFrame  \n",
    "        ECG 檔案對應資料表，需包含 'SUBJECT_ID', 'T0', 'T1', 'T1_lead2', 'PREFIX', 'FOLDER', 'HEADER', 'Total_lead2_sec' 等欄位。\n",
    "\n",
    "    Return:\n",
    "    - base_match_ecg_df: pd.DataFrame  \n",
    "        新增 ECG 資訊的 base 資料，每筆 row 增加 ECG 欄位（無符合者為 None），並自動儲存結果 csv，回傳完整 DataFrame。\n",
    "    \"\"\"\n",
    "    for col in [\"T0\",\"T1\",\"T1_lead2\"]:\n",
    "        ecg_match_info[col] = pd.to_datetime(ecg_match_info[col],errors=\"raise\")\n",
    "\n",
    "    for col in [\"INTIME\",\"OUTTIME\"]:\n",
    "        base[col] = pd.to_datetime(base[col],errors=\"raise\")\n",
    "\n",
    "\n",
    "    PREFIX = []\n",
    "    FOLDER= []\n",
    "    HEADER= []\n",
    "    T0= []\n",
    "    T1= []\n",
    "    T1_lead2= []\n",
    "    Total_lead2_sec= []\n",
    "    match_count = 0\n",
    "    for _,row in base.iterrows():\n",
    "        ecgs = ecg_match_info[ecg_match_info['SUBJECT_ID']==row['SUBJECT_ID']] # May have many ecg records\n",
    "        intime = row['INTIME']\n",
    "        outtime = row['OUTTIME']\n",
    "        overlaps = (\n",
    "            (ecgs['T0'].clip(lower=intime) <\n",
    "            ecgs['T1_lead2'].clip(upper=outtime))\n",
    "        )\n",
    "        overlaps_match = overlaps[overlaps==True]\n",
    "\n",
    "        if len(overlaps_match)>1:\n",
    "            # 多筆 ECG Header Match: 擷取最後一段\n",
    "            multple_ecg_headers = ecg_match_info.iloc[overlaps_match.index]\n",
    "            latest_row = multple_ecg_headers.sort_values(by='T1_lead2', ascending=False).iloc[0]\n",
    "            PREFIX.append(latest_row['PREFIX'])\n",
    "            FOLDER.append(latest_row['FOLDER'])\n",
    "            HEADER.append(latest_row['HEADER'])\n",
    "            T0.append(latest_row['T0'])\n",
    "            T1.append(latest_row['T1'])\n",
    "            T1_lead2.append(latest_row['T1_lead2'])\n",
    "            Total_lead2_sec.append(latest_row['Total_lead2_sec'])\n",
    "            match_count+=1\n",
    "        elif len(overlaps_match) == 1:\n",
    "            idx = overlaps_match.index[0]                # 抓第一個 index（scalar）\n",
    "            latest_row = ecg_match_info.iloc[idx]        # 現在才會回傳 Series\n",
    "            PREFIX.append(latest_row['PREFIX'])\n",
    "            FOLDER.append(latest_row['FOLDER'])\n",
    "            HEADER.append(latest_row['HEADER'])\n",
    "            T0.append(latest_row['T0'])\n",
    "            T1.append(latest_row['T1'])\n",
    "            T1_lead2.append(latest_row['T1_lead2'])\n",
    "            Total_lead2_sec.append(latest_row['Total_lead2_sec'])\n",
    "            match_count+=1\n",
    "        else:\n",
    "            PREFIX.append(None)\n",
    "            FOLDER.append(None)\n",
    "            HEADER.append(None)\n",
    "            T0.append(None)\n",
    "            T1.append(None)\n",
    "            T1_lead2.append(None)\n",
    "            Total_lead2_sec.append(None)\n",
    "\n",
    "\n",
    "    base_match_ecg_df = base.copy()\n",
    "    base_match_ecg_df['PREFIX'] = PREFIX\n",
    "    base_match_ecg_df['FOLDER'] = FOLDER\n",
    "    base_match_ecg_df['HEADER'] = HEADER\n",
    "    base_match_ecg_df['T0'] = T0\n",
    "    base_match_ecg_df['T1'] = T1\n",
    "    base_match_ecg_df['T1_lead2'] = T1_lead2\n",
    "    base_match_ecg_df['Total_lead2_sec'] = Total_lead2_sec\n",
    "\n",
    "    base_match_ecg_df.to_csv(os.path.join(LOGS,\"base_match_ecg_df.csv\"),index=False)\n",
    "    print(f\"Total Records {match_count} matched!\")\n",
    "    return base_match_ecg_df\n",
    "ecg_match_info = pd.read_csv(os.path.join(LOGS,\"ecg_match_info.csv\"))\n",
    "base_match_ecg_df=match_base_ecg(base,ecg_match_info)\n",
    "\n",
    "#刪除沒有 match的 row 方便作分析\n",
    "base_match_ecg_df = base_match_ecg_df.dropna(subset=['T0','T1','T1_lead2'], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7d2f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2237 entries, 25 to 15755\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   SUBJECT_ID            2237 non-null   int64         \n",
      " 1   HADM_ID               2237 non-null   int64         \n",
      " 2   ICUSTAY_ID            2237 non-null   int64         \n",
      " 3   INTIME                2237 non-null   datetime64[ns]\n",
      " 4   OUTTIME               2237 non-null   datetime64[ns]\n",
      " 5   ADMITTIME             2237 non-null   object        \n",
      " 6   DISCHTIME             2237 non-null   object        \n",
      " 7   DEATHTIME             403 non-null    object        \n",
      " 8   HOSPITAL_EXPIRE_FLAG  2237 non-null   int64         \n",
      " 9   HAS_AF                2237 non-null   float64       \n",
      " 10  PREFIX                2237 non-null   object        \n",
      " 11  FOLDER                2237 non-null   object        \n",
      " 12  HEADER                2237 non-null   object        \n",
      " 13  T0                    2237 non-null   datetime64[ns]\n",
      " 14  T1                    2237 non-null   datetime64[ns]\n",
      " 15  T1_lead2              2237 non-null   datetime64[ns]\n",
      " 16  Total_lead2_sec       2237 non-null   float64       \n",
      "dtypes: datetime64[ns](5), float64(2), int64(4), object(6)\n",
      "memory usage: 314.6+ KB\n",
      "Missing : set()\n",
      "\n",
      "Exception Patient(Match LEADII < 36000):\n",
      "7848     30047\n",
      "13696    74913\n",
      "15379    81475\n",
      "Name: SUBJECT_ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_match_ecg_df.info()\n",
    "print(f\"Missing : {total_set - set(base_match_ecg_df['SUBJECT_ID'].to_list())}\")\n",
    "\n",
    "filtered_df = base_match_ecg_df[\n",
    "    base_match_ecg_df['SUBJECT_ID'].isin(total_set) &\n",
    "    (base_match_ecg_df['Total_lead2_sec'] < 36000)\n",
    "]\n",
    "\n",
    "print(f\"\\nException Patient(Match LEADII < 36000):\\n{filtered_df['SUBJECT_ID']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53118fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2237\n",
      "=============== Current Error Tolerance : 0 min ===============\n",
      "Missing (Out of ICU OUTTIME)(len : 22): [5685, 14197, 19866, 26055, 26732, 27428, 27884, 30170, 48777, 51277, 55402, 56751, 64944, 65055, 69144, 72282, 75476, 81475, 85866, 86968, 88521, 89091]\n",
      "Missing (Before  ICU INTIME)(len : 31): [1354, 2172, 10013, 14266, 16258, 16463, 17667, 19718, 22337, 43400, 44437, 45608, 45942, 47543, 48011, 50762, 54209, 60659, 68607, 73760, 75886, 75998, 79709, 80254, 83598, 84142, 84845, 84874, 92777, 93031, 93898]\n",
      "Missing (Out of ADM DISCHTIME)(len : 27): [6605, 7251, 9258, 14266, 16463, 24626, 26271, 28180, 30047, 32012, 43084, 43673, 51277, 55725, 57299, 57968, 72048, 74913, 75883, 77689, 81475, 81593, 84914, 85979, 88503, 94351, 99067]\n",
      "Missing (Before  ADM ADMITTIME)(len : 0): []\n",
      "Outliter - ICU INTIME earlier than ADMIITIME (44): [2514, 10320, 10342, 11591, 15569, 18572, 32067, 41902, 42093, 42721, 43060, 45806, 46802, 50141, 52802, 54147, 54429, 55616, 60074, 60852, 64374, 64384, 71491, 72160, 74805, 76800, 78221, 79589, 81063, 85393, 86245, 87769, 88982, 89195, 90680, 90942, 91181, 91603, 91625, 92235, 93159, 93479, 93638, 94753]\n",
      "Outlier intersect with total set (13): [11591, 18572, 45806, 46802, 60074, 64384, 87769, 89195, 90680, 90942, 91181, 91603, 92235]\n",
      "Outliter - ICU OUTTIME later than DISCHTIME (117): {76801, 2063, 7695, 28180, 69146, 95776, 9258, 24626, 81475, 9289, 46154, 78410, 43084, 51277, 16463, 7251, 73299, 44123, 53856, 75883, 57968, 74866, 16499, 88691, 70273, 66189, 94351, 54929, 43673, 19102, 26271, 74913, 96937, 3242, 68780, 50353, 81593, 59585, 4802, 44742, 59085, 97488, 50385, 66770, 95957, 90846, 21219, 23780, 61667, 45801, 83691, 46837, 11512, 31993, 24825, 99067, 74503, 9993, 32012, 30477, 21775, 29967, 90396, 97565, 26398, 83751, 99115, 95022, 18738, 10552, 25915, 92475, 62782, 52034, 46927, 93528, 40798, 30047, 78691, 1892, 73068, 72048, 77689, 20858, 91004, 55679, 98182, 81810, 75670, 31127, 23450, 28073, 55725, 3506, 22962, 12212, 23474, 63925, 84914, 88503, 14266, 56264, 6605, 23503, 54736, 57299, 94164, 46041, 85979, 95201, 81378, 74727, 61928, 60393, 65513, 97773, 42995}\n",
      "Outlier intersect with total set (27): [6605, 7251, 9258, 14266, 16463, 24626, 26271, 28180, 30047, 32012, 43084, 43673, 51277, 55725, 57299, 57968, 72048, 74913, 75883, 77689, 81475, 81593, 84914, 85979, 88503, 94351, 99067]\n"
     ]
    }
   ],
   "source": [
    "# base_match_ecg_df 時間分析: 分析 T0/T1_lead2 的時間段，與'ADMITTIME','DISCHTIME','DEATHTIME','INTIME', 'OUTTIME'等欄位的交叉狀況與異常數值分析\n",
    "# 考量記錄誤差，可以加入時間誤差容忍 : tolerance_min, tolerance\n",
    "# 目前預設 30 (以輸出為準)\n",
    "# 避免紀錄誤差刪去太多紀錄：時間誤差容忍　30 mins\n",
    "\n",
    "\n",
    "for col in ['ADMITTIME','DISCHTIME','DEATHTIME','INTIME', 'OUTTIME', 'T0', 'T1','T1_lead2']:\n",
    "    base_match_ecg_df[col] = pd.to_datetime(base_match_ecg_df[col], errors='coerce')\n",
    "print(len(base_match_ecg_df.dropna(subset=['ADMITTIME','DISCHTIME','INTIME', 'OUTTIME', 'T0', 'T1','T1_lead2'])))\n",
    "\n",
    "# 1. ECG: T0/T1_Lead2 是否落在 ICU INTIME/OUTTIME\n",
    "tolerance_min = 0\n",
    "tolerance = pd.Timedelta(minutes=tolerance_min)\n",
    "print(f\"=============== Current Error Tolerance : {tolerance_min} min ===============\")\n",
    "in_ICU_OUTTIME = (\n",
    "    # (base_match_ecg_df['T0'] >= (base_match_ecg_df['INTIME'] - tolerance)) &\n",
    "    (base_match_ecg_df['T1_lead2'] <= (base_match_ecg_df['OUTTIME'] + tolerance))\n",
    ")\n",
    "\n",
    "in_ICU_INTIME = (\n",
    "    (base_match_ecg_df['T0'] >= (base_match_ecg_df['INTIME'] - tolerance))\n",
    ")\n",
    "\n",
    "ecg_in_ICU_OUT = base_match_ecg_df[in_ICU_OUTTIME]\n",
    "ecg_in_ICU_OUT_set = set(ecg_in_ICU_OUT['SUBJECT_ID'].to_list())\n",
    "ecg_in_ICU_IN = base_match_ecg_df[in_ICU_INTIME]\n",
    "ecg_in_ICU_IN_set = set(ecg_in_ICU_IN['SUBJECT_ID'].to_list())\n",
    "print(f\"Missing (Out of ICU OUTTIME)(len : {len(total_set - ecg_in_ICU_OUT_set)}): {sorted(list(total_set - ecg_in_ICU_OUT_set))}\")\n",
    "print(f\"Missing (Before  ICU INTIME)(len : {len(total_set - ecg_in_ICU_IN_set)}): {sorted(list(total_set - ecg_in_ICU_IN_set))}\")\n",
    "\n",
    "# 2. ECG: T0/T1_Lead2 是否落在 ADMISSIONS ADMITTIME/DISCHTIME\n",
    "in_ADM_ADMITTIME = (\n",
    "    (base_match_ecg_df['T0'] >= (base_match_ecg_df['ADMITTIME'] - tolerance))\n",
    ")\n",
    "\n",
    "in_ADM_DISCHTIME = (\n",
    "    (base_match_ecg_df['T1_lead2'] <= (base_match_ecg_df['DISCHTIME'] + tolerance))\n",
    ")\n",
    "\n",
    "ecg_in_ADM_OUT = base_match_ecg_df[in_ADM_DISCHTIME]\n",
    "ecg_in_ADM_OUT_set = set(ecg_in_ADM_OUT['SUBJECT_ID'].to_list())\n",
    "ecg_in_ADM_IN = base_match_ecg_df[in_ADM_ADMITTIME]\n",
    "ecg_in_ADM_IN_set = set(ecg_in_ADM_IN['SUBJECT_ID'].to_list())\n",
    "\n",
    "print(f\"Missing (Out of ADM DISCHTIME)(len : {len(total_set - ecg_in_ADM_OUT_set)}): {sorted(list(total_set - ecg_in_ADM_OUT_set))}\")\n",
    "print(f\"Missing (Before  ADM ADMITTIME)(len : {len(total_set - ecg_in_ADM_IN_set)}): {sorted(list(total_set - ecg_in_ADM_IN_set))}\")\n",
    "\n",
    "# 3 Outlier Analysis: \n",
    "# 3.1 所有 ICU INTIME 都小於 ADM　ADMITTIME (30min tolerance)\n",
    "intime_smaller_than_admittime = (\n",
    "    (base_match_ecg_df['INTIME']+tolerance >= (base_match_ecg_df['ADMITTIME']))\n",
    ")\n",
    "outlier_intime_earlier_than_admittime = base_match_ecg_df[~intime_smaller_than_admittime]\n",
    "outlier_intime_earlier_than_admittime_set = set(outlier_intime_earlier_than_admittime['SUBJECT_ID'].to_list())\n",
    "print(f\"Outliter - ICU INTIME earlier than ADMIITIME ({len(outlier_intime_earlier_than_admittime)}): {outlier_intime_earlier_than_admittime['SUBJECT_ID'].to_list()}\")\n",
    "print(f\"Outlier intersect with total set ({len(total_set & outlier_intime_earlier_than_admittime_set)}): {sorted(list(total_set & outlier_intime_earlier_than_admittime_set)) }\")\n",
    "\n",
    "#3.2 所有 ICU OUTTIME 都早於 ADM DISCHTIME (30min tolerance)\n",
    "outtime_smaller_than_dischtime = (\n",
    "    (base_match_ecg_df['T1_lead2'] <= (base_match_ecg_df['DISCHTIME'] + tolerance))\n",
    ")\n",
    "outlier_outtime_later_than_dischtime = base_match_ecg_df[~outtime_smaller_than_dischtime]\n",
    "outlier_outtime_later_than_dischtime_set = set(outlier_outtime_later_than_dischtime['SUBJECT_ID'].to_list())\n",
    "print(f\"Outliter - ICU OUTTIME later than DISCHTIME ({len(outlier_outtime_later_than_dischtime_set)}): {outlier_outtime_later_than_dischtime_set}\")\n",
    "print(f\"Outlier intersect with total set ({len(total_set & outlier_outtime_later_than_dischtime_set)}): {sorted(list(total_set & outlier_outtime_later_than_dischtime_set)) }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d85105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Current Error Tolerance : 0 min ===============\n",
      "Total Surv: 1384\n",
      "Total Mort:285\n",
      "重疊 subject id : set()\n"
     ]
    }
   ],
   "source": [
    "# 劃分存活組 Surv, 死亡組 Mort, 尚未過濾沒有 Output \n",
    "def split_surv_and_mort(base_match_ecg_df:pd.DataFrame)->Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    依據 ICU 住院紀錄與 ECG 訊號時間範圍，將資料依存活（survival）與死亡（mortality）組進行嚴格分組。\n",
    "    分組與過濾條件如下：\n",
    "\n",
    "    - **ECG 覆蓋條件（兩組皆需同時滿足，允許 30 分鐘誤差 tolerance）：**\n",
    "        設 ICU 期間為 [INTIME, OUTTIME]，ECG 覆蓋期間為 [T0, T1_lead2]\n",
    "        - (1) T0 >= INTIME - tolerance\n",
    "        - (2) T1_lead2 <= OUTTIME + tolerance\n",
    "\n",
    "    - **死亡組（mort）定義：**\n",
    "        - HOSPITAL_EXPIRE_FLAG == 1\n",
    "        - DEATHTIME 不為空（即已知死亡時間）\n",
    "        - ECG 覆蓋條件成立（如上）\n",
    "        - ECG 訊號長度 Total_lead2_sec >= 36,000（即總共有10小時長度的訊號，可以不連續）\n",
    "\n",
    "        公式：\n",
    "        ```\n",
    "        mort = base_match_ecg_df[\n",
    "            (T0 >= INTIME - tolerance) &\n",
    "            (T1_lead2 <= OUTTIME + tolerance) &\n",
    "            (HOSPITAL_EXPIRE_FLAG == 1) &\n",
    "            (DEATHTIME 非空) &\n",
    "            (Total_lead2_sec >= 36000)\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "    - **存活組（surv）定義：**\n",
    "        - HOSPITAL_EXPIRE_FLAG == 0\n",
    "        - DEATHTIME 為空（即未於住院期間死亡）\n",
    "        - ECG 覆蓋條件成立（如上）\n",
    "        - ECG 訊號長度 Total_lead2_sec >= 36,000（即連續10小時）\n",
    "\n",
    "        公式：\n",
    "        ```\n",
    "        surv = base_match_ecg_df[\n",
    "            (T0 >= INTIME - tolerance) &\n",
    "            (T1_lead2 <= OUTTIME + tolerance) &\n",
    "            (HOSPITAL_EXPIRE_FLAG == 0) &\n",
    "            (DEATHTIME 為空) &\n",
    "            (Total_lead2_sec >= 36000)\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "    Args:\n",
    "    - base_match_ecg_df: pd.DataFrame  \n",
    "        已合併 ICU 與 ECG 資訊的 DataFrame，需包含 ICU 入出時間、ECG 開始結束時間、死亡標記等必要欄位。\n",
    "\n",
    "    Return:\n",
    "    - surv: pd.DataFrame  \n",
    "        存活組資料表（完全滿足上述存活條件）。\n",
    "    - mort: pd.DataFrame  \n",
    "        死亡組資料表（完全滿足上述死亡條件）。\n",
    "\n",
    "    篩選過程中會自動轉換時間欄位型別、去除不齊全資料，並輸出分組人數與 SUBJECT_ID 是否有重疊。\n",
    "    \"\"\"\n",
    "   \n",
    "    for col in ['ADMITTIME','DISCHTIME','DEATHTIME','INTIME', 'OUTTIME', 'T0', 'T1','T1_lead2']:\n",
    "        base_match_ecg_df[col] = pd.to_datetime(base_match_ecg_df[col], errors='coerce')\n",
    "        \n",
    "    base_match_ecg_df.dropna(subset=['ADMITTIME','DISCHTIME','INTIME', 'OUTTIME', 'T0', 'T1','T1_lead2'],inplace=True)\n",
    "    # 1. ECG: T0/T1_Lead2 是否落在 ICU INTIME/OUTTIME (with 30min 誤差容忍)\n",
    "    tolerance_min = 0\n",
    "    tolerance = pd.Timedelta(minutes=tolerance_min)\n",
    "    print(f\"=============== Current Error Tolerance : {tolerance_min} min ===============\")\n",
    "\n",
    "    in_ICU_OUTTIME = (\n",
    "        # (base_match_ecg_df['T0'] >= (base_match_ecg_df['INTIME'] - tolerance)) &\n",
    "        (base_match_ecg_df['T1_lead2'] <= (base_match_ecg_df['OUTTIME'] + tolerance))\n",
    "    )\n",
    "\n",
    "    in_ICU_INTIME = (\n",
    "        (base_match_ecg_df['T0'] >= (base_match_ecg_df['INTIME'] - tolerance))\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    #split group\n",
    "    mort = base_match_ecg_df[\n",
    "        (in_ICU_OUTTIME) &\n",
    "        (in_ICU_INTIME) &\n",
    "        (base_match_ecg_df['HOSPITAL_EXPIRE_FLAG'] == 1) &\n",
    "        (~base_match_ecg_df['DEATHTIME'].isnull()) &\n",
    "        (base_match_ecg_df['Total_lead2_sec'] >= 36000)\n",
    "    ]\n",
    "\n",
    "    surv = base_match_ecg_df[\n",
    "        (in_ICU_OUTTIME) &\n",
    "        (in_ICU_INTIME) &\n",
    "        (base_match_ecg_df['HOSPITAL_EXPIRE_FLAG']==0) &\n",
    "        (base_match_ecg_df['DEATHTIME'].isnull()) &\n",
    "        (base_match_ecg_df['Total_lead2_sec'] >= 36000)\n",
    "    ]\n",
    "\n",
    "    #分析有無重疊\n",
    "    print(f\"Total Surv: {len(surv)}\\nTotal Mort:{len(mort)}\")\n",
    "    print(f\"重疊 subject id : {set(mort['SUBJECT_ID'].to_list()) & set(surv['SUBJECT_ID'].to_list())}\")\n",
    "\n",
    "    return surv, mort\n",
    "\n",
    "surv, mort=split_surv_and_mort(base_match_ecg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357a5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dob_and_calculate_age(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    將輸入資料（df）根據 SUBJECT_ID 合併 PATIENTS.csv 內的 DOB 欄位，並計算各病患的入院時年齡（AGE_YEARS），\n",
    "    以及標記是否小於 125 歲（AGE_UNDER_125）。同時檢查資料合理性（ADMITTIME 必須晚於 DOB），\n",
    "    並於過程中列印無效日期的樣本資料供檢查。\n",
    "\n",
    "    處理步驟與產出：\n",
    "    1. 合併 df 與 PATIENTS.csv（以 SUBJECT_ID 對應），並強制轉換日期格式。\n",
    "    2. 計算 ADMITTIME 與 DOB 差的年數，存為 AGE_YEARS 欄位。\n",
    "    3. 標記年齡是否小於 125 歲（AGE_UNDER_125，布林值）。\n",
    "    4. 列印 ADMITTIME <= DOB 的異常樣本前 5 筆供檢查。\n",
    "    5. 回傳已新增年齡資訊之完整 DataFrame。\n",
    "\n",
    "    Args:\n",
    "    - df: pd.DataFrame  \n",
    "        欲補齊出生日期與計算年齡的主資料表，必須包含 'SUBJECT_ID' 及 'ADMITTIME' 欄位。\n",
    "\n",
    "    Return:\n",
    "    - df_with_Age: pd.DataFrame  \n",
    "        回傳合併後的資料表，新增欄位：\n",
    "        - 'DOB': 病患出生日期\n",
    "        - 'AGE_YEARS': 入院時年齡（以年計）\n",
    "        - 'AGE_UNDER_125': 是否小於 125 歲（布林值）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pat = pd.read_csv(os.path.join(DATA_CSV, \"PATIENTS.csv\"),\n",
    "                          usecols=['SUBJECT_ID', 'DOB','GENDER'])\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Required CSV files not found in {DATA_CSV}\")\n",
    "\n",
    "    pat['DOB'] = pd.to_datetime(pat['DOB'], errors='coerce')\n",
    "    pat[\"GENDER\"] = pat[\"GENDER\"].map({\"M\": 0, \"F\": 1})\n",
    "\n",
    "    if pat['GENDER'].isnull().any():\n",
    "        print(f\"Error: patient Gender Missing\")\n",
    "        return\n",
    "    df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'], errors='coerce')\n",
    "\n",
    "    print(f\"Before Merge, Total id : {df['SUBJECT_ID'].nunique()}\")\n",
    "    df_with_Age = df.merge(pat, on=\"SUBJECT_ID\", how=\"left\")\n",
    "    df_with_Age = df_with_Age.reset_index(drop=True)\n",
    "    print(f\"After Merge, Total id : {df_with_Age['SUBJECT_ID'].nunique()}\")\n",
    "    \n",
    "\n",
    "    # 加入檢查 & 過濾\n",
    "    mask_valid = (\n",
    "        (df_with_Age['ADMITTIME'] > df_with_Age['DOB'])\n",
    "    )\n",
    "    print(\"Invalid date rows:\")\n",
    "    print(df_with_Age[~mask_valid][['SUBJECT_ID', 'DOB', 'ADMITTIME']].head(5))\n",
    "\n",
    "    df_with_Age['AGE_YEARS'] = df_with_Age['ADMITTIME'].dt.year - df_with_Age['DOB'].dt.year\n",
    "    df_with_Age['AGE_UNDER_125'] = (df_with_Age['AGE_YEARS'] < 125)\n",
    "\n",
    "\n",
    "\n",
    "    return df_with_Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68a1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Merge, Total id : 285\n",
      "After Merge, Total id : 285\n",
      "Invalid date rows:\n",
      "Empty DataFrame\n",
      "Columns: [SUBJECT_ID, DOB, ADMITTIME]\n",
      "Index: []\n",
      "容忍誤差: 0 days 00:00:00\n",
      "174\n",
      "Total Mort Post ECG(50) : {98182, 32012, 30477, 2063, 21775, 94351, 28180, 75670, 43673, 69146, 19102, 26271, 28073, 9258, 99115, 55725, 18738, 24626, 84914, 88503, 10552, 81593, 25915, 92475, 9289, 43084, 23503, 46927, 66770, 57299, 95957, 85979, 90846, 53856, 95201, 81378, 21219, 61667, 74727, 80744, 60393, 65513, 75883, 73068, 57968, 72048, 74866, 42995, 77689, 99067}\n",
      "dead_42731 with post ecg (19):{32012, 94351, 28180, 43673, 26271, 9258, 55725, 24626, 84914, 88503, 81593, 43084, 57299, 85979, 75883, 57968, 72048, 77689, 99067}\n",
      "Final Mort : 75\n",
      "Missing(42) : [2172, 9258, 10013, 13123, 14266, 14772, 16258, 16463, 19603, 24626, 26271, 27860, 28180, 30047, 30887, 32012, 43084, 43673, 48011, 51277, 55725, 57299, 57968, 60659, 63552, 65268, 72048, 74913, 75883, 77689, 79709, 81475, 81593, 81694, 84142, 84845, 84914, 85979, 88503, 93031, 94351, 99067]\n",
      "Addtion(13) : [6017, 14094, 20124, 25131, 29470, 46092, 51951, 59222, 67976, 68391, 80375, 80737, 95839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_423412\\1659443203.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# 死亡組分析\n",
    "# Output: Stage 2 最終結果，detailed 保留所有資訊 , filtered 過濾掉不合格資料，僅保留全部符合的 data\n",
    "# 1. mort_stage2_detailed.csv\n",
    "# 2. mort_stage2_filtered.csv\n",
    "mort_with_age = merge_dob_and_calculate_age(mort)\n",
    "\n",
    "# Post ECG :T1_lead2 出現在死亡後或出現後\n",
    "print(f\"容忍誤差: {tolerance}\")\n",
    "post_ecg_icu = (\n",
    "    ((mort_with_age['DEATHTIME']+tolerance )< mort_with_age['T1_lead2'])\n",
    ")\n",
    "\n",
    "post_ecg_dischtime = (\n",
    "    ((mort_with_age['DEATHTIME']+tolerance) < mort_with_age['DISCHTIME'])\n",
    ")\n",
    "\n",
    "mort_with_post_ecg = mort_with_age[(post_ecg_icu | post_ecg_dischtime)]\n",
    "mort_with_post_ecg_set = set(mort_with_post_ecg['SUBJECT_ID'].to_list())\n",
    "\n",
    "# Diff between T1_lead2 and DEATHTIME\n",
    "mask = (\n",
    "    ((mort_with_age['DEATHTIME']-mort_with_age['T1_lead2']).abs())> pd.Timedelta(seconds=1800)\n",
    ")\n",
    "print(len(mort_with_age[mask]))\n",
    "print(f\"Total Mort Post ECG({len(mort_with_post_ecg_set)}) : {mort_with_post_ecg_set}\")\n",
    "print(f\"dead_42731 with post ecg ({len(dead_set & mort_with_post_ecg_set)}):{dead_set & mort_with_post_ecg_set}\")\n",
    "\n",
    "mort_with_age['post_ecg_icu'] = post_ecg_icu\n",
    "mort_with_age['post_ecg_dischtime'] = post_ecg_dischtime\n",
    "mort_with_age.to_csv(os.path.join(LOGS,\"mort_stage2_detailed.csv\"),index=False)\n",
    "\n",
    "# 過濾: Age under 125 and without post ecg(採用嚴格定義：deathtime 必須晚於 T1_lead2結束或離院時間(取小，且 ICU OUTTIME 已經在 Split 時檢查過這邊不再檢查))\n",
    "\n",
    "mort_final = mort_with_age[\n",
    "    (mort_with_age['AGE_UNDER_125']) &\n",
    "    (~(post_ecg_icu | post_ecg_dischtime)) &\n",
    "    (~mask)\n",
    "]\n",
    "mort_final.to_csv(os.path.join(LOGS,\"mort_stage2_filtered.csv\"),index=False)\n",
    "mort_final_set = set(mort_final['SUBJECT_ID'].to_list())\n",
    "\n",
    "print(f\"Final Mort : {len(mort_final)}\")\n",
    "print(f\"Missing({len(dead_set-mort_final_set)}) : {sorted(list(dead_set-mort_final_set))}\")\n",
    "print(f\"Addtion({len(mort_final_set-dead_set)}) : {sorted(list(mort_final_set-dead_set))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e1d230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tolerance : 0 days 00:00:00\n",
      "Before Merge, Total id : 1384\n",
      "After Merge, Total id : 1384\n",
      "Invalid date rows:\n",
      "Empty DataFrame\n",
      "Columns: [SUBJECT_ID, DOB, ADMITTIME]\n",
      "Index: []\n",
      "Total Surv Post ECG(17) : {70273, 4802, 76801, 74503, 61928, 9993, 45801, 78410, 83751, 6605, 54736, 50385, 7251, 16499, 94164, 11512, 24825}\n",
      "alive_42731 with post ecg (2):{7251, 6605}\n",
      "594\n",
      "Final Surv : 732\n",
      "Missing(42) : [1354, 5685, 6605, 7251, 14197, 17667, 19718, 19866, 22337, 26055, 26732, 27428, 27884, 30170, 43400, 44437, 45608, 45942, 47543, 48777, 50762, 54209, 55402, 56751, 64944, 65055, 68607, 69144, 72282, 73760, 75476, 75886, 75998, 80254, 83598, 84874, 85866, 86968, 88521, 89091, 92777, 93898]\n",
      "Addtion(408) : [217, 735, 1144, 1313, 1418, 1449, 2442, 2747, 2754, 3515, 4180, 4420, 4477, 4788, 4807, 6039, 6063, 6090, 6229, 7470, 7965, 8105, 8109, 9398, 9664, 9685, 9987, 10042, 10257, 10534, 10581, 10924, 11641, 11728, 12094, 12400, 12831, 13049, 13759, 13970, 14914, 15021, 15538, 15885, 16196, 17976, 18357, 18413, 18998, 19342, 19375, 19999, 20479, 20763, 20795, 20860, 20984, 21187, 21448, 22104, 22383, 22393, 22418, 22461, 22782, 23061, 23413, 23584, 23674, 24063, 24567, 24856, 25016, 25452, 26377, 26781, 27192, 27423, 28416, 28660, 28897, 29073, 29343, 29463, 29664, 29826, 30457, 30484, 30542, 30775, 31051, 31559, 40624, 40797, 41373, 41619, 41882, 42310, 42663, 42702, 43060, 43601, 43729, 43866, 43911, 43926, 44044, 44298, 44408, 44454, 44570, 44666, 44784, 44789, 45186, 45199, 45816, 45838, 45936, 46237, 46268, 46339, 46467, 46471, 46797, 46817, 46910, 47203, 47216, 47233, 47473, 47634, 47715, 47718, 47749, 48006, 48058, 48145, 48640, 48666, 48674, 49685, 49739, 49872, 50110, 50537, 50643, 50649, 50729, 50793, 50816, 50832, 50888, 50915, 51045, 51072, 51121, 51466, 51660, 51722, 51942, 52039, 52199, 52363, 52592, 52791, 52952, 53342, 53541, 53731, 53876, 54073, 54195, 54197, 54221, 54397, 54735, 54900, 55473, 55616, 56201, 56443, 56802, 56947, 56996, 57023, 57308, 57815, 57905, 58022, 58113, 58205, 58270, 58310, 58377, 58938, 58993, 59469, 59757, 59788, 60272, 60343, 60424, 60949, 60969, 61441, 61956, 62032, 62603, 62721, 63201, 63237, 63525, 63559, 63563, 63750, 63755, 63771, 63773, 64112, 64230, 64280, 64411, 65370, 65537, 66067, 66326, 66571, 66654, 66692, 66717, 66807, 66919, 67239, 67415, 67653, 68401, 68564, 68724, 68909, 68916, 69082, 69265, 69591, 69675, 69681, 70329, 70451, 70494, 70740, 70745, 71328, 71491, 71857, 72000, 72439, 72592, 72790, 72999, 73686, 74230, 74514, 74546, 74610, 74639, 74851, 75029, 75155, 75160, 75509, 75772, 75779, 75972, 76001, 76066, 76116, 76476, 76698, 77227, 77520, 77578, 77686, 77807, 78010, 78050, 78431, 78948, 78979, 79032, 79163, 79283, 79589, 80030, 80313, 80586, 80675, 81041, 81203, 81295, 81408, 81766, 81875, 81885, 81893, 82021, 82115, 82179, 82565, 82609, 82847, 82928, 83129, 83857, 83887, 83908, 83981, 84776, 85171, 85393, 85757, 86245, 86318, 86355, 86381, 86411, 86589, 86662, 86692, 86965, 87203, 87239, 87275, 87754, 87817, 87975, 88013, 88117, 88734, 88747, 88982, 89148, 90158, 90410, 90917, 91309, 91579, 91635, 92613, 92764, 93159, 93208, 93272, 93378, 93638, 94103, 94645, 94753, 94768, 94840, 94896, 94993, 95107, 95542, 95603, 95948, 96225, 96259, 96261, 96361, 96445, 96567, 96574, 96594, 96697, 96732, 96965, 97028, 97232, 97448, 97834, 97984, 98016, 98046, 98434, 98494, 98565, 98643, 98733, 98957, 98959, 98961, 99011, 99038, 99088, 99216, 99291, 99412, 99439, 99545, 99560, 99564, 99616, 99645, 99708, 99759, 99796, 99830, 99922]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louislin\\AppData\\Local\\Temp\\ipykernel_423412\\1659443203.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "#存活組分析\n",
    "# Output: Stage 2 最終結果，detailed 保留所有資訊 , filtered 過濾掉不合格資料，僅保留全部符合的 data\n",
    "# 1. surv_stage2_detailed.csv\n",
    "# 2. surv_stage2_filtered.csv\n",
    "print(f\"Current tolerance : {tolerance}\")\n",
    "surv_with_age = merge_dob_and_calculate_age(surv)\n",
    "\n",
    "# Post Discharge ECG signal : T1_lead2 >= DISCHTIME or OUTTIME\n",
    "surv_post_ecg_icu = (\n",
    "    surv_with_age['T1_lead2'] >= (surv_with_age['OUTTIME'] + tolerance)\n",
    ")\n",
    "\n",
    "surv_post_ecg_dischtime = (\n",
    "    surv_with_age['T1_lead2'] >= (surv_with_age['DISCHTIME'] + tolerance)\n",
    ")\n",
    "\n",
    "surv_early_ecg_icu = (\n",
    "    surv_with_age['T0'] <= (surv_with_age['INTIME'] - tolerance)\n",
    ")\n",
    "\n",
    "surv_early_ecg_admittime = (\n",
    "    surv_with_age['T0'] <= (surv_with_age['ADMITTIME'] - tolerance)\n",
    ")\n",
    "\n",
    "surv_with_post_ecg = surv_with_age[(surv_post_ecg_icu | surv_post_ecg_dischtime)]\n",
    "surv_with_post_ecg_set = set(surv_with_post_ecg['SUBJECT_ID'].to_list())\n",
    "\n",
    "print(f\"Total Surv Post ECG({len(surv_with_post_ecg)}) : {surv_with_post_ecg_set}\")\n",
    "print(f\"alive_42731 with post ecg ({len(alive_set & surv_with_post_ecg_set)}):{alive_set & surv_with_post_ecg_set}\")\n",
    "\n",
    "mask = (\n",
    "    (surv_with_age['OUTTIME'] - surv_with_age['T1_lead2']).abs() > pd.Timedelta(seconds=1800)\n",
    ")\n",
    "print(len(surv_with_age[mask]))\n",
    "\n",
    "surv_with_age['post_ecg_icu'] = surv_post_ecg_icu\n",
    "surv_with_age['post_ecg_dischtime'] = surv_post_ecg_dischtime\n",
    "\n",
    "surv_with_age.to_csv(os.path.join(LOGS,\"surv_stage2_detailed.csv\"),index=False)\n",
    "\n",
    "# 過濾: Age under 125 and without post ecg(採用嚴格定義：T1_lead2 必須早於 ICU OUTTIME或離院時間(取小))，並且不會有住院前或進ICU前 ICU 紀錄\n",
    "\n",
    "surv_final = surv_with_age[\n",
    "    (surv_with_age['AGE_UNDER_125']) &\n",
    "    (~((surv_post_ecg_icu | surv_post_ecg_dischtime))) &\n",
    "    (~((surv_early_ecg_icu | surv_early_ecg_admittime))) &\n",
    "    (~mask)\n",
    "]\n",
    "surv_final.to_csv(os.path.join(LOGS,\"surv_stage2_filtered.csv\"),index=False)\n",
    "surv_final_set = set(surv_final['SUBJECT_ID'].to_list())\n",
    "\n",
    "print(f\"Final Surv : {len(surv_final)}\")\n",
    "print(f\"Missing({len(alive_set - surv_final_set)}) : {sorted(list(alive_set - surv_final_set))}\")\n",
    "print(f\"Addtion({len(surv_final_set-alive_set)}) : {sorted(list(surv_final_set-alive_set))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surv Addtion Analysis : 這邊以後是跟原檔案(42731)交叉對比，不用管\n",
    "cols = ['SUBJECT_ID','HADM_ID','ICUSTAY_ID','INTIME','OUTTIME','ADMITTIME','DISCHTIME','T0','T1_lead2','Total_lead2_sec','HEADER']\n",
    "surv_final_addition = surv_final[surv_final['SUBJECT_ID'].isin(surv_final_set-alive_set)][cols]\n",
    "surv_final_addition.to_csv(os.path.join(LOGS,\"surv_stage2_addition.csv\"),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08d4dd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SUBJECT_ID  HADM_ID  in_df2\n",
      "0         12372   176291    True\n",
      "1          1409   124337    True\n",
      "2         14186   171902    True\n",
      "3         15218   159946    True\n",
      "4         16353   152248    True\n",
      "..          ...      ...     ...\n",
      "361       94811   177259    True\n",
      "362       94853   197339    True\n",
      "363       95313   185643    True\n",
      "364        9537   128113    True\n",
      "365       96148   135552    True\n",
      "\n",
      "[366 rows x 3 columns]\n",
      "有缺漏\n",
      "以下組合沒在df2出現：\n",
      "     SUBJECT_ID  HADM_ID\n",
      "7         19866   168234\n",
      "13        26055   196340\n",
      "14        27884   128997\n",
      "19        30170   120612\n",
      "27        43400   124673\n",
      "40        47543   184250\n",
      "86        65055   153824\n",
      "95        68607   124149\n",
      "99        69144   153453\n",
      "110       73760   141240\n",
      "112       75886   175305\n",
      "132       83598   107203\n",
      "137       84874   101728\n",
      "141       86968   191498\n",
      "147       88521   166573\n",
      "170        1354   144830\n",
      "174       14197   170569\n",
      "182       17667   197869\n",
      "185       19718   178054\n",
      "187       22337   125014\n",
      "195       26732   125708\n",
      "198       27428   108193\n",
      "222       44437   181557\n",
      "224       45608   131072\n",
      "225       45942   107293\n",
      "232       48777   189754\n",
      "240       50762   194353\n",
      "247       54209   122588\n",
      "249       55402   192684\n",
      "252       56751   151131\n",
      "253        5685   193803\n",
      "282       64944   152631\n",
      "287        6605   147165\n",
      "300       72282   173521\n",
      "301        7251   189182\n",
      "305       75476   197785\n",
      "308       75998   109866\n",
      "316       80254   134373\n",
      "335       85866   168584\n",
      "344       89091   135118\n",
      "354       92777   114815\n",
      "357       93898   132413\n"
     ]
    }
   ],
   "source": [
    "# Surv: [subject_id, hadm_id] 嚴格交叉比對\n",
    "cols = ['SUBJECT_ID', 'HADM_ID']\n",
    "surv_final_same = surv_final[surv_final['SUBJECT_ID'].isin(alive_set & surv_final_set)]\n",
    "\n",
    "set_df2 = set(zip(surv_final_same['SUBJECT_ID'], surv_final_same['HADM_ID']))\n",
    "\n",
    "alive_yuran['in_df2'] = list(zip(alive_yuran['SUBJECT_ID'], alive_yuran['HADM_ID']))\n",
    "alive_yuran['in_df2'] = alive_yuran['in_df2'].apply(lambda x: x in set_df2)\n",
    "\n",
    "print(alive_yuran[['SUBJECT_ID', 'HADM_ID', 'in_df2']])\n",
    "\n",
    "all_in = alive_yuran['in_df2'].all()\n",
    "print(\"全部都有對應\" if all_in else \"有缺漏\")\n",
    "\n",
    "not_in_df2 = alive_yuran[~alive_yuran['in_df2']]\n",
    "print(\"以下組合沒在df2出現：\")\n",
    "print(not_in_df2[['SUBJECT_ID','HADM_ID']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27950743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SUBJECT_ID  HADM_ID  in_df2\n",
      "0          1049   117138    True\n",
      "1         10629   193631    True\n",
      "2         16463   183203   False\n",
      "3         16924   193729    True\n",
      "4         18875   189045    True\n",
      "..          ...      ...     ...\n",
      "99         9258   183354   False\n",
      "100       94351   166930   False\n",
      "101       96746   140835    True\n",
      "102       97801   165472    True\n",
      "103       99067   130940   False\n",
      "\n",
      "[104 rows x 3 columns]\n",
      "有缺漏\n",
      "以下組合沒在df2出現：\n",
      "     SUBJECT_ID  HADM_ID\n",
      "2         16463   183203\n",
      "7         26271   191492\n",
      "10        30047   172127\n",
      "13        51277   103045\n",
      "16        57968   179996\n",
      "21        81475   101662\n",
      "22        81593   192553\n",
      "24        84142   133986\n",
      "26        84914   198880\n",
      "27        88503   100784\n",
      "29        93031   125979\n",
      "30        10013   165520\n",
      "33        14266   111581\n",
      "37        16258   148188\n",
      "42         2172   181252\n",
      "43        24626   167669\n",
      "47        28180   119762\n",
      "52        32012   138125\n",
      "55        43084   169551\n",
      "57        43673   121255\n",
      "60        48011   166293\n",
      "64        55725   163060\n",
      "65        57299   196002\n",
      "69        60659   133081\n",
      "81        72048   145205\n",
      "83        74913   180080\n",
      "85        75883   155525\n",
      "87        77689   136766\n",
      "88        79709   158151\n",
      "91        84845   105748\n",
      "93        85979   132989\n",
      "99         9258   183354\n",
      "100       94351   166930\n",
      "103       99067   130940\n"
     ]
    }
   ],
   "source": [
    "# Mort: [subject_id, hadm_id] 嚴格交叉比對\n",
    "cols = ['SUBJECT_ID', 'HADM_ID']\n",
    "mort_final_same = mort_final[mort_final['SUBJECT_ID'].isin(dead_set & mort_final_set)]\n",
    "\n",
    "set_df2 = set(zip(mort_final_same['SUBJECT_ID'], mort_final_same['HADM_ID']))\n",
    "\n",
    "dead_yuran['in_df2'] = list(zip(dead_yuran['SUBJECT_ID'], dead_yuran['HADM_ID']))\n",
    "dead_yuran['in_df2'] = dead_yuran['in_df2'].apply(lambda x: x in set_df2)\n",
    "\n",
    "print(dead_yuran[['SUBJECT_ID', 'HADM_ID', 'in_df2']])\n",
    "\n",
    "all_in = dead_yuran['in_df2'].all()\n",
    "print(\"全部都有對應\" if all_in else \"有缺漏\")\n",
    "\n",
    "not_in_df2 = dead_yuran[~dead_yuran['in_df2']]\n",
    "print(\"以下組合沒在df2出現：\")\n",
    "print(not_in_df2[['SUBJECT_ID','HADM_ID']])\n",
    "# Missing in Mort: [2172, 24626, 30047, 48011, 51277, 74913, 81475, 84142]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
